# -*- coding: utf-8 -*-
"""eval.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1seul5CbqXyoZprocqRMqn8T4JTI8DfTM
"""

import argparse
import torch
from torch.utils.data import DataLoader
from torchvision import transforms
import torch.nn as nn
import torch.nn.functional as F
from torchvision.models import mobilenet_v3_small

# import sys
# sys.argv = [
#     'eval.py',
#     '--weights', 'multitask_model.pt',
#     '--data_root', 'data',
#     '--tasks', 'all'
# ]

class MultiTaskHead(nn.Module):
    def __init__(self, in_channels, num_det_classes=10, num_seg_classes=21, num_cls_classes=10):
        super().__init__()

        # 共享特徵提取層 (符合參數限制)
        self.shared_conv = nn.Sequential(
            nn.Conv2d(in_channels, 256, 3, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 128, 3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True)
        )

        # 分割專用上採樣路徑 (新增部分)
        self.seg_upsample = nn.Sequential(
            nn.Upsample(scale_factor=32, mode='bilinear', align_corners=True),  # 關鍵修正
            nn.Conv2d(128, num_seg_classes, 1)  # 輸出通道數=類別數
        )

        # 檢測與分類輸出層
        self.det_cls_conv = nn.Conv2d(128, 6 + num_det_classes + num_cls_classes, 1)

        self.num_det_classes = num_det_classes
        self.num_cls_classes = num_cls_classes

    def forward(self, x):
        shared_feat = self.shared_conv(x)

        # 分割輸出 (512x512)
        seg_output = self.seg_upsample(shared_feat)  # [B,21,512,512]

        # 檢測與分類輸出
        det_cls_output = self.det_cls_conv(shared_feat)
        det_output = det_cls_output[:, :6+self.num_det_classes]
        cls_output = F.adaptive_avg_pool2d(
            det_cls_output[:, 6+self.num_det_classes:], (1, 1)
        ).squeeze(-1).squeeze(-1)

        return det_output, seg_output, cls_output


class MultiTaskModel(nn.Module):
    def __init__(self):
        super(MultiTaskModel, self).__init__()

        # 使用 MobileNetV3-Small 作為骨幹網路
        self.backbone = mobilenet_v3_small(pretrained=True)
        self.backbone.classifier = nn.Identity()  # 移除分類頭

        # 簡單的頸部網路
        self.neck = nn.Sequential(
            nn.Conv2d(576, 256, 3, padding=1),  # MobileNetV3-Small 輸出通道數
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True)
        )

        # 多任務頭部
        self.head = MultiTaskHead(256)

    def forward(self, x):
        # 骨幹網路特徵提取
        features = self.backbone.features(x)

        # 頸部處理
        neck_feat = self.neck(features)

        # 多任務輸出
        det_out, seg_out, cls_out = self.head(neck_feat)

        return det_out, seg_out, cls_out

import os
import torch
from torch.utils.data import Dataset, DataLoader
from PIL import Image
import json
import torchvision.transforms as transforms
from pycocotools.coco import COCO
import numpy as np

class MultiTaskDataset(Dataset):
    def __init__(self, data_root, task_type, split='train', transform=None):
        self.data_root = data_root
        self.task_type = task_type
        self.split = split
        self.transform = transform
        self.samples = []

        # 根據任務類型載入對應的數據
        if task_type == 'segmentation':
            self.data_dir = os.path.join(data_root, 'mini_voc_seg', split)
            self._load_segmentation_samples()
        elif task_type == 'detection':
            self.data_dir = os.path.join(data_root, 'mini_coco_det', split)
            self._load_detection_samples()
        elif task_type == 'classification':
            self.data_dir = os.path.join(data_root, 'imagenette_160', split)
            self._load_classification_samples()
        else:
            raise ValueError(f"不支援的任務類型: {task_type}")

    def _load_segmentation_samples(self):
        """載入分割任務的樣本 (VOC格式)"""
        if not os.path.exists(self.data_dir):
            raise FileNotFoundError(f"找不到目錄: {self.data_dir}")

        # 獲取所有 .jpg 圖片文件
        for filename in os.listdir(os.path.join(self.data_dir, "images")):
            if filename.endswith('.jpg'):
                img_path = os.path.join(self.data_dir, "images", filename)
                # 對應的 mask 文件 (.png)
                mask_filename = filename.replace('.jpg', '.png')
                mask_path = os.path.join(self.data_dir, "annotations", mask_filename)
                # print(mask_path)

                if os.path.exists(mask_path):
                    self.samples.append({
                        'image_path': img_path,
                        'mask_path': mask_path,
                        'filename': filename
                    })

        print(f"載入分割樣本: {len(self.samples)} 個")

    def _load_detection_samples(self):
        """載入檢測任務的樣本 (COCO格式)"""
        if not os.path.exists(self.data_dir):
            raise FileNotFoundError(f"找不到目錄: {self.data_dir}")

        # 載入 COCO 格式的標註文件
        annotation_file = os.path.join(self.data_dir, "annotations", 'annotations.json')
        if not os.path.exists(annotation_file):
            raise FileNotFoundError(f"找不到標註文件: {annotation_file}")

        self.coco = COCO(annotation_file)
        self.image_ids = list(self.coco.imgs.keys())

        for img_id in self.image_ids:
            img_info = self.coco.imgs[img_id]
            img_path = os.path.join(self.data_dir, "images", img_info['file_name'])

            # 獲取該圖片的所有標註
            ann_ids = self.coco.getAnnIds(imgIds=img_id)
            annotations = self.coco.loadAnns(ann_ids)

            self.samples.append({
                'image_path': img_path,
                'image_id': img_id,
                'annotations': annotations,
                'image_info': img_info
            })

        print(f"載入檢測樣本: {len(self.samples)} 個")

    def _load_classification_samples(self):
        """載入分類任務的樣本 (Imagenette格式)"""
        if not os.path.exists(self.data_dir):
            raise FileNotFoundError(f"找不到目錄: {self.data_dir}")

        # 載入標籤文件
        labels_file = os.path.join(self.data_dir, "annotations", 'labels.json')
        if not os.path.exists(labels_file):
            raise FileNotFoundError(f"找不到標籤文件: {labels_file}")

        with open(labels_file, 'r') as f:
            labels_data = json.load(f)

        self.classes = labels_data['classes']
        self.class_to_idx = labels_data['class_to_idx']

        for label_info in labels_data['labels']:
            img_path = os.path.join(self.data_dir, "images", label_info['filename'])

            self.samples.append({
                'image_path': img_path,
                'class_name': label_info['class_name'],
                'class_id': label_info['class_id'],
                'filename': label_info['filename']
            })

        print(f"載入分類樣本: {len(self.samples)} 個")

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        sample = self.samples[idx]

        # 載入圖片
        image = Image.open(sample['image_path']).convert('RGB')

        if self.task_type == 'segmentation':
            # 分割任務：返回圖片和分割 mask
            mask = Image.open(sample['mask_path'])

            if self.transform:
                image = self.transform(image)
                # mask 需要特殊處理，不能使用 normalize
                mask = mask.resize((512, 512), Image.NEAREST)
                mask = torch.from_numpy(np.array(mask)).long()  # int64 (long)
                # print("get item",torch.unique(mask))
            else:
                mask = np.array(mask)
            return image, mask

        elif self.task_type == 'detection':
            # 檢測任務：返回圖片和邊界框標註
            annotations = sample['annotations']

            # 處理邊界框和標籤
            boxes = []
            labels = []

            for ann in annotations:
                # COCO格式: [x, y, width, height]
                bbox = ann['bbox']
                # 轉換為 [x1, y1, x2, y2] 格式
                x1, y1, w, h = bbox
                x2, y2 = x1 + w, y1 + h
                boxes.append([x1, y1, x2, y2])
                labels.append(ann['category_id'])

            # 套用轉換 (需同步處理影像和邊界框)
            if self.transform:
                # 獲取原始影像尺寸
                orig_w, orig_h = image.size

                # 套用影像轉換
                image = self.transform(image)

                # 計算縮放比例 (假設transform包含Resize到固定尺寸)
                new_h, new_w = image.shape[1], image.shape[2]  # C,H,W格式

                # 調整邊界框座標
                scale_x = new_w / orig_w
                scale_y = new_h / orig_h
                boxes = torch.tensor(boxes, dtype=torch.float32)
                if len(boxes) > 0:
                    boxes[:, [0, 2]] *= scale_x
                    boxes[:, [1, 3]] *= scale_y

            # 處理空標註情況
            if len(boxes) == 0:
                boxes = torch.zeros((0, 4), dtype=torch.float32)
                labels = torch.zeros((0,), dtype=torch.int64)
            else:
                boxes = torch.tensor(boxes, dtype=torch.float32)
                labels = torch.tensor(labels, dtype=torch.int64)

            target = {
                'boxes': boxes,
                'labels': labels,
                'image_id': torch.tensor([sample['image_id']], dtype=torch.int64)
            }

            return image, target

        elif self.task_type == 'classification':
            # 分類任務：返回圖片和類別標籤
            label = sample['class_id']

            if self.transform:
                image = self.transform(image)

            return image, torch.tensor(label, dtype=torch.long)

# 不同任務的資料轉換
def get_transforms(task_type, split='train'):
    """根據任務類型獲取對應的資料轉換"""

    if task_type == 'classification':
        if split == 'train':
            return transforms.Compose([
                transforms.Resize((512, 512)),
                transforms.RandomHorizontalFlip(p=0.5),
                transforms.ToTensor(),
                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
            ])
        else:
            return transforms.Compose([
                transforms.Resize((512, 512)),
                transforms.ToTensor(),
                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
            ])

    elif task_type == 'segmentation':
        return transforms.Compose([
            transforms.Resize((512, 512)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])

    elif task_type == 'detection':
        return transforms.Compose([
            transforms.Resize((512, 512)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])


def detection_collate(batch):
    images = []
    targets = []

    for image, target in batch:
        images.append(image)
        targets.append(target)

    images = torch.stack(images, dim=0)
    return images, targets


# ====== 評估函數 ======

def evaluate_detection(model, dataloader, device):
    from torchmetrics.detection.mean_ap import MeanAveragePrecision
    model.eval()
    metric = MeanAveragePrecision(iou_type='bbox').to(device)
    with torch.no_grad():
        for images, targets in dataloader:
            images = images.to(device)
            det_out, _, _ = model(images)
            preds = []
            for i in range(images.size(0)):
                boxes, scores, labels = parse_detection_output(det_out[i])
                # 關鍵：全部移到 device
                preds.append({
                    'boxes': boxes.to(device),
                    'scores': scores.to(device),
                    'labels': labels.to(device)
                })
            # target 也要全部移到 device
            targets = [
                {
                    'boxes': t['boxes'].to(device),
                    'labels': t['labels'].to(device)
                } for t in targets
            ]
            metric.update(preds, targets)
    return metric.compute()['map'].item()

def evaluate_segmentation(model, dataloader, device):
    model.eval()
    total_iou = 0
    count = 0
    with torch.no_grad():
        for images, masks in dataloader:
            images = images.to(device)
            masks = masks.to(device)
            _, seg_out, _ = model(images)
            preds = torch.argmax(seg_out, dim=1)
            # mIoU計算（簡單平均）
            intersection = ((preds == masks) & (masks != 255)).sum().item()
            union = ((preds != 255) | (masks != 255)).sum().item()
            total_iou += intersection / (union + 1e-8)
            count += 1
    return total_iou / count if count > 0 else 0.0

def evaluate_classification(model, dataloader, device):
    model.eval()
    correct_top1 = 0
    total = 0
    with torch.no_grad():
        for images, labels in dataloader:
            images = images.to(device)
            labels = labels.to(device)
            _, _, cls_out = model(images)
            preds = torch.argmax(cls_out, dim=1)
            correct_top1 += (preds == labels).sum().item()
            total += labels.size(0)
    return correct_top1 / total if total > 0 else 0.0

# ====== 假設 parse_detection_output 已根據你的模型格式實作 ======
def parse_detection_output(det_output, conf_threshold=0.5, num_classes=10):
    """
    解析检测输出（假设输出形状为 [16, H, W]）
    det_output: 模型输出的检测头结果，形状 [16, H, W]
    """
    # 分离参数
    cx = det_output[0]   # 中心x坐标 [H, W]
    cy = det_output[1]   # 中心y坐标 [H, W]
    w = det_output[2]    # 宽度 [H, W]
    h = det_output[3]    # 高度 [H, W]
    conf = det_output[4].sigmoid()  # 置信度 [H, W]
    cls_probs = det_output[5:5+num_classes].softmax(dim=0)  # 类别概率 [num_classes, H, W]

    # 生成网格坐标
    grid_h, grid_w = cx.shape
    y_grid, x_grid = torch.meshgrid(
        torch.arange(grid_h, device=det_output.device),
        torch.arange(grid_w, device=det_output.device),
        indexing='ij'
    )

    # 转换为绝对坐标（假设输入图像尺寸为 512x512）
    scale = 512 / grid_h  # 特征图到原图的缩放比例
    x1 = (x_grid + cx - w/2) * scale
    y1 = (y_grid + cy - h/2) * scale
    x2 = (x_grid + cx + w/2) * scale
    y2 = (y_grid + cy + h/2) * scale

    # 展平所有预测
    boxes = torch.stack([x1, y1, x2, y2], dim=-1).reshape(-1, 4)  # [H*W, 4]
    confidences = conf.reshape(-1)                                # [H*W]
    class_ids = cls_probs.permute(1,2,0).reshape(-1, num_classes).argmax(dim=1)  # [H*W]

    # 过滤低置信度预测
    mask = confidences > conf_threshold
    return boxes[mask], confidences[mask], class_ids[mask]

# ====== 主程式 ======
def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--weights',  default='./multitask_model.pt', type=str, required=True)
    parser.add_argument('--data_root', default='./data', type=str, required=True)
    parser.add_argument('--tasks', type=str, default='all', choices=['all', 'segmentation', 'detection', 'classification'])
    args = parser.parse_args()

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    # 載入模型
    model = MultiTaskModel()  # 請根據你的模型替換
    model.load_state_dict(torch.load(args.weights, map_location=device))
    model.to(device)

    # 載入資料集
    seg_loader = DataLoader(
      MultiTaskDataset(
        data_root=args.data_root,
        task_type='segmentation',
        split='val',
        transform=get_transforms('segmentation', 'val')
    ), batch_size=8, shuffle=True) if args.tasks in ['all', 'segmentation'] else None
    det_loader = DataLoader(
      MultiTaskDataset(
        data_root=args.data_root,
        task_type='detection',
        split='val',
        transform=get_transforms('detection', 'val')
    ), batch_size=8,  collate_fn=detection_collate) if args.tasks in ['all', 'detection'] else None
    cls_loader = DataLoader(
      MultiTaskDataset(
        data_root=args.data_root,
        task_type='classification',
        split='val',
        transform=get_transforms('classification', 'val')
    ), batch_size=32) if args.tasks in ['all', 'classification'] else None

    # 執行評估
    if args.tasks in ['all', 'detection'] and det_loader is not None:
        mAP = evaluate_detection(model, det_loader, device)
        print(f"mAP: {mAP:.4f}")

    if args.tasks in ['all', 'segmentation'] and seg_loader is not None:
        mIoU = evaluate_segmentation(model, seg_loader, device)
        print(f"mIoU: {mIoU:.4f}")

    if args.tasks in ['all', 'classification'] and cls_loader is not None:
        top1 = evaluate_classification(model, cls_loader, device)
        print(f"Top-1 Accuracy: {top1:.4f}")

if __name__ == '__main__':
    main()
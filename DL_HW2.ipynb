{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "_jbL5YEyQo4F",
        "9jgasRy6qW0b"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install fiftyone pycocotools scikit-multilearn\n",
        "!pip install tqdm"
      ],
      "metadata": {
        "id": "TMc0N947rPeH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98278845-6035-4a2d-e4d4-846bdbb5d7d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fiftyone\n",
            "  Downloading fiftyone-1.5.2-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: pycocotools in /usr/local/lib/python3.11/dist-packages (2.0.9)\n",
            "Collecting scikit-multilearn\n",
            "  Downloading scikit_multilearn-0.2.0-py3-none-any.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: aiofiles in /usr/local/lib/python3.11/dist-packages (from fiftyone) (24.1.0)\n",
            "Collecting argcomplete (from fiftyone)\n",
            "  Downloading argcomplete-3.6.2-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting async_lru>=2 (from fiftyone)\n",
            "  Downloading async_lru-2.0.5-py3-none-any.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from fiftyone) (4.13.4)\n",
            "Collecting boto3 (from fiftyone)\n",
            "  Downloading boto3-1.38.36-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.11/dist-packages (from fiftyone) (5.5.2)\n",
            "Collecting dacite<1.8.0,>=1.6.0 (from fiftyone)\n",
            "  Downloading dacite-1.7.0-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from fiftyone) (0.3.7)\n",
            "Collecting Deprecated (from fiftyone)\n",
            "  Downloading Deprecated-1.2.18-py2.py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting ftfy (from fiftyone)\n",
            "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.11/dist-packages (from fiftyone) (4.12.3)\n",
            "Collecting hypercorn>=0.13.2 (from fiftyone)\n",
            "  Downloading hypercorn-0.17.3-py3-none-any.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: Jinja2>=3 in /usr/local/lib/python3.11/dist-packages (from fiftyone) (3.1.6)\n",
            "Collecting kaleido!=0.2.1.post1 (from fiftyone)\n",
            "  Downloading kaleido-0.2.1-py2.py3-none-manylinux1_x86_64.whl.metadata (15 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from fiftyone) (3.10.0)\n",
            "Collecting mongoengine~=0.29.1 (from fiftyone)\n",
            "  Downloading mongoengine-0.29.1-py3-none-any.whl.metadata (6.7 kB)\n",
            "Collecting motor~=3.6.0 (from fiftyone)\n",
            "  Downloading motor-3.6.1-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from fiftyone) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from fiftyone) (24.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from fiftyone) (2.2.2)\n",
            "Requirement already satisfied: Pillow>=6.2 in /usr/local/lib/python3.11/dist-packages (from fiftyone) (11.2.1)\n",
            "Requirement already satisfied: plotly>=4.14 in /usr/local/lib/python3.11/dist-packages (from fiftyone) (5.24.1)\n",
            "Collecting pprintpp (from fiftyone)\n",
            "  Downloading pprintpp-0.4.0-py2.py3-none-any.whl.metadata (7.9 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from fiftyone) (5.9.5)\n",
            "Collecting pymongo~=4.9.2 (from fiftyone)\n",
            "  Downloading pymongo-4.9.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.11/dist-packages (from fiftyone) (2025.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from fiftyone) (6.0.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from fiftyone) (2024.11.6)\n",
            "Collecting retrying (from fiftyone)\n",
            "  Downloading retrying-1.3.4-py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting rtree (from fiftyone)\n",
            "  Downloading rtree-1.4.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from fiftyone) (1.6.1)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.11/dist-packages (from fiftyone) (0.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from fiftyone) (1.15.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from fiftyone) (75.2.0)\n",
            "Collecting sseclient-py<2,>=1.7.2 (from fiftyone)\n",
            "  Downloading sseclient_py-1.8.0-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting sse-starlette<1,>=0.10.3 (from fiftyone)\n",
            "  Downloading sse_starlette-0.10.3-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: starlette>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from fiftyone) (0.46.2)\n",
            "Collecting strawberry-graphql>=0.262.4 (from fiftyone)\n",
            "  Downloading strawberry_graphql-0.273.0-py3-none-any.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.11/dist-packages (from fiftyone) (0.9.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from fiftyone) (4.67.1)\n",
            "Collecting xmltodict (from fiftyone)\n",
            "  Downloading xmltodict-0.14.2-py2.py3-none-any.whl.metadata (8.0 kB)\n",
            "Collecting universal-analytics-python3<2,>=1.0.1 (from fiftyone)\n",
            "  Downloading universal_analytics_python3-1.1.1-py3-none-any.whl.metadata (5.5 kB)\n",
            "Collecting pydash (from fiftyone)\n",
            "  Downloading pydash-8.0.5-py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting fiftyone-brain<0.22,>=0.21.2 (from fiftyone)\n",
            "  Downloading fiftyone_brain-0.21.2-py3-none-any.whl.metadata (4.8 kB)\n",
            "Collecting fiftyone-db<2.0,>=0.4 (from fiftyone)\n",
            "  Downloading fiftyone_db-1.1.7.tar.gz (7.9 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting voxel51-eta<0.15,>=0.14.0 (from fiftyone)\n",
            "  Downloading voxel51_eta-0.14.2-py2.py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.11/dist-packages (from fiftyone) (4.11.0.86)\n",
            "Requirement already satisfied: h11 in /usr/local/lib/python3.11/dist-packages (from hypercorn>=0.13.2->fiftyone) (0.16.0)\n",
            "Requirement already satisfied: h2>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from hypercorn>=0.13.2->fiftyone) (4.2.0)\n",
            "Collecting priority (from hypercorn>=0.13.2->fiftyone)\n",
            "  Downloading priority-2.0.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting wsproto>=0.14.0 (from hypercorn>=0.13.2->fiftyone)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2>=3->fiftyone) (3.0.2)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly>=4.14->fiftyone) (9.1.2)\n",
            "Collecting dnspython<3.0.0,>=1.16.0 (from pymongo~=4.9.2->fiftyone)\n",
            "  Downloading dnspython-2.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette>=0.24.0->fiftyone) (4.9.0)\n",
            "Collecting graphql-core<3.4.0,>=3.2.0 (from strawberry-graphql>=0.262.4->fiftyone)\n",
            "  Downloading graphql_core-3.2.6-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: python-dateutil<3.0,>=2.7 in /usr/local/lib/python3.11/dist-packages (from strawberry-graphql>=0.262.4->fiftyone) (2.9.0.post0)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from strawberry-graphql>=0.262.4->fiftyone) (4.14.0)\n",
            "Requirement already satisfied: httpx>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from universal-analytics-python3<2,>=1.0.1->fiftyone) (0.28.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.11/dist-packages (from voxel51-eta<0.15,>=0.14.0->fiftyone) (1.0.0)\n",
            "Requirement already satisfied: glob2 in /usr/local/lib/python3.11/dist-packages (from voxel51-eta<0.15,>=0.14.0->fiftyone) (0.7)\n",
            "Collecting jsonlines (from voxel51-eta<0.15,>=0.14.0->fiftyone)\n",
            "  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting py7zr (from voxel51-eta<0.15,>=0.14.0->fiftyone)\n",
            "  Downloading py7zr-1.0.0-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting rarfile (from voxel51-eta<0.15,>=0.14.0->fiftyone)\n",
            "  Downloading rarfile-4.2-py3-none-any.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from voxel51-eta<0.15,>=0.14.0->fiftyone) (2.32.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from voxel51-eta<0.15,>=0.14.0->fiftyone) (1.17.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.11/dist-packages (from voxel51-eta<0.15,>=0.14.0->fiftyone) (2.4.0)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.11/dist-packages (from voxel51-eta<0.15,>=0.14.0->fiftyone) (5.3.1)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.11/dist-packages (from voxel51-eta<0.15,>=0.14.0->fiftyone) (2.4.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->fiftyone) (2.7)\n",
            "Collecting botocore<1.39.0,>=1.38.36 (from boto3->fiftyone)\n",
            "  Downloading botocore-1.38.36-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3->fiftyone)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting s3transfer<0.14.0,>=0.13.0 (from boto3->fiftyone)\n",
            "  Downloading s3transfer-0.13.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.11/dist-packages (from Deprecated->fiftyone) (1.17.2)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy->fiftyone) (0.2.13)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->fiftyone) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->fiftyone) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->fiftyone) (4.58.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->fiftyone) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->fiftyone) (3.2.3)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->fiftyone) (2025.2)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from scikit-image->fiftyone) (3.5)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image->fiftyone) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image->fiftyone) (2025.6.1)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image->fiftyone) (0.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->fiftyone) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->fiftyone) (3.6.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette>=0.24.0->fiftyone) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette>=0.24.0->fiftyone) (1.3.1)\n",
            "Requirement already satisfied: hyperframe<7,>=6.1 in /usr/local/lib/python3.11/dist-packages (from h2>=3.1.0->hypercorn>=0.13.2->fiftyone) (6.1.0)\n",
            "Requirement already satisfied: hpack<5,>=4.1 in /usr/local/lib/python3.11/dist-packages (from h2>=3.1.0->hypercorn>=0.13.2->fiftyone) (4.1.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.10.0->universal-analytics-python3<2,>=1.0.1->fiftyone) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.10.0->universal-analytics-python3<2,>=1.0.1->fiftyone) (1.0.9)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonlines->voxel51-eta<0.15,>=0.14.0->fiftyone) (25.3.0)\n",
            "Collecting texttable (from py7zr->voxel51-eta<0.15,>=0.14.0->fiftyone)\n",
            "  Downloading texttable-1.7.0-py2.py3-none-any.whl.metadata (9.8 kB)\n",
            "Requirement already satisfied: pycryptodomex>=3.20.0 in /usr/local/lib/python3.11/dist-packages (from py7zr->voxel51-eta<0.15,>=0.14.0->fiftyone) (3.23.0)\n",
            "Collecting brotli>=1.1.0 (from py7zr->voxel51-eta<0.15,>=0.14.0->fiftyone)\n",
            "  Downloading Brotli-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
            "Collecting pyzstd>=0.16.1 (from py7zr->voxel51-eta<0.15,>=0.14.0->fiftyone)\n",
            "  Downloading pyzstd-0.17.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
            "Collecting pyppmd<1.3.0,>=1.1.0 (from py7zr->voxel51-eta<0.15,>=0.14.0->fiftyone)\n",
            "  Downloading pyppmd-1.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.4 kB)\n",
            "Collecting pybcj<1.1.0,>=1.0.0 (from py7zr->voxel51-eta<0.15,>=0.14.0->fiftyone)\n",
            "  Downloading pybcj-1.0.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.7 kB)\n",
            "Collecting multivolumefile>=0.2.3 (from py7zr->voxel51-eta<0.15,>=0.14.0->fiftyone)\n",
            "  Downloading multivolumefile-0.2.3-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting inflate64<1.1.0,>=1.0.0 (from py7zr->voxel51-eta<0.15,>=0.14.0->fiftyone)\n",
            "  Downloading inflate64-1.0.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->voxel51-eta<0.15,>=0.14.0->fiftyone) (3.4.2)\n",
            "Downloading fiftyone-1.5.2-py3-none-any.whl (10.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m61.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scikit_multilearn-0.2.0-py3-none-any.whl (89 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.4/89.4 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading async_lru-2.0.5-py3-none-any.whl (6.1 kB)\n",
            "Downloading dacite-1.7.0-py3-none-any.whl (12 kB)\n",
            "Downloading fiftyone_brain-0.21.2-py3-none-any.whl (112 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.3/112.3 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hypercorn-0.17.3-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.7/61.7 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kaleido-0.2.1-py2.py3-none-manylinux1_x86_64.whl (79.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.9/79.9 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mongoengine-0.29.1-py3-none-any.whl (112 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.4/112.4 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading motor-3.6.1-py3-none-any.whl (74 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.8/74.8 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pymongo-4.9.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m52.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sse_starlette-0.10.3-py3-none-any.whl (8.0 kB)\n",
            "Downloading sseclient_py-1.8.0-py2.py3-none-any.whl (8.8 kB)\n",
            "Downloading strawberry_graphql-0.273.0-py3-none-any.whl (303 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m303.8/303.8 kB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading universal_analytics_python3-1.1.1-py3-none-any.whl (10 kB)\n",
            "Downloading voxel51_eta-0.14.2-py2.py3-none-any.whl (943 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m943.0/943.0 kB\u001b[0m \u001b[31m59.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading argcomplete-3.6.2-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading boto3-1.38.36-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.9/139.9 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Deprecated-1.2.18-py2.py3-none-any.whl (10.0 kB)\n",
            "Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pprintpp-0.4.0-py2.py3-none-any.whl (16 kB)\n",
            "Downloading pydash-8.0.5-py3-none-any.whl (102 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.1/102.1 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading retrying-1.3.4-py3-none-any.whl (11 kB)\n",
            "Downloading rtree-1.4.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (541 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m541.1/541.1 kB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xmltodict-0.14.2-py2.py3-none-any.whl (10.0 kB)\n",
            "Downloading botocore-1.38.36-py3-none-any.whl (13.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.6/13.6 MB\u001b[0m \u001b[31m61.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dnspython-2.7.0-py3-none-any.whl (313 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.6/313.6 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphql_core-3.2.6-py3-none-any.whl (203 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.4/203.4 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading s3transfer-0.13.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.2/85.2 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Downloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
            "Downloading priority-2.0.0-py3-none-any.whl (8.9 kB)\n",
            "Downloading py7zr-1.0.0-py3-none-any.whl (69 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.7/69.7 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rarfile-4.2-py3-none-any.whl (29 kB)\n",
            "Downloading Brotli-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading inflate64-1.0.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (96 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.4/96.4 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multivolumefile-0.2.3-py3-none-any.whl (17 kB)\n",
            "Downloading pybcj-1.0.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.7/50.7 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyppmd-1.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.3/141.3 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyzstd-0.17.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (412 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m412.9/412.9 kB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading texttable-1.7.0-py2.py3-none-any.whl (10 kB)\n",
            "Building wheels for collected packages: fiftyone-db\n",
            "  Building wheel for fiftyone-db (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fiftyone-db: filename=fiftyone_db-1.1.7-py3-none-manylinux1_x86_64.whl size=42156244 sha256=d2e07e670207188cda4e057c10b090f58be71772d416d2fd58634359c3b6b15b\n",
            "  Stored in directory: /root/.cache/pip/wheels/bf/75/da/54cd52c92db9f0c4e88ca5efdbac2304b49927249c9c859b8d\n",
            "Successfully built fiftyone-db\n",
            "Installing collected packages: texttable, sseclient-py, scikit-multilearn, pprintpp, kaleido, brotli, xmltodict, wsproto, rtree, retrying, rarfile, pyzstd, pyppmd, pydash, pybcj, priority, multivolumefile, jsonlines, jmespath, inflate64, graphql-core, ftfy, fiftyone-db, dnspython, Deprecated, dacite, async_lru, argcomplete, strawberry-graphql, pymongo, py7zr, hypercorn, botocore, voxel51-eta, universal-analytics-python3, sse-starlette, s3transfer, motor, mongoengine, fiftyone-brain, boto3, fiftyone\n",
            "Successfully installed Deprecated-1.2.18 argcomplete-3.6.2 async_lru-2.0.5 boto3-1.38.36 botocore-1.38.36 brotli-1.1.0 dacite-1.7.0 dnspython-2.7.0 fiftyone-1.5.2 fiftyone-brain-0.21.2 fiftyone-db-1.1.7 ftfy-6.3.1 graphql-core-3.2.6 hypercorn-0.17.3 inflate64-1.0.3 jmespath-1.0.1 jsonlines-4.0.0 kaleido-0.2.1 mongoengine-0.29.1 motor-3.6.1 multivolumefile-0.2.3 pprintpp-0.4.0 priority-2.0.0 py7zr-1.0.0 pybcj-1.0.6 pydash-8.0.5 pymongo-4.9.2 pyppmd-1.2.0 pyzstd-0.17.0 rarfile-4.2 retrying-1.3.4 rtree-1.4.0 s3transfer-0.13.0 scikit-multilearn-0.2.0 sse-starlette-0.10.3 sseclient-py-1.8.0 strawberry-graphql-0.273.0 texttable-1.7.0 universal-analytics-python3-1.1.1 voxel51-eta-0.14.2 wsproto-1.2.0 xmltodict-0.14.2\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset dowmload"
      ],
      "metadata": {
        "id": "lI3sk-EZIRxO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## COCO dataset"
      ],
      "metadata": {
        "id": "EWp658eXPZUn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import fiftyone as fo\n",
        "import fiftyone.zoo as foz\n",
        "import json\n",
        "import os\n",
        "from collections import defaultdict\n",
        "import random\n",
        "\n",
        "# 選擇 10 個常見的 COCO 類別\n",
        "selected_classes = [\n",
        "    \"person\", \"car\", \"bicycle\", \"motorcycle\", \"airplane\",\n",
        "    \"bus\", \"train\", \"truck\", \"boat\", \"dog\"\n",
        "]\n",
        "\n",
        "# 下載指定類別的 COCO 2017 數據集\n",
        "dataset = foz.load_zoo_dataset(\n",
        "    \"coco-2017\",\n",
        "    splits=[\"train\", \"validation\"],\n",
        "    label_types=[\"detections\"],\n",
        "    classes=selected_classes,\n",
        "    max_samples=350  # 下載足夠的樣本以便後續篩選\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tOyorGDZIMNt",
        "outputId": "9478f333-0745-4153-b8a2-d914c78c4893"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading split 'train' to '/root/fiftyone/coco-2017/train' if necessary\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.zoo.datasets:Downloading split 'train' to '/root/fiftyone/coco-2017/train' if necessary\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading annotations to '/root/fiftyone/coco-2017/tmp-download/annotations_trainval2017.zip'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.utils.coco:Downloading annotations to '/root/fiftyone/coco-2017/tmp-download/annotations_trainval2017.zip'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 100% |██████|    1.9Gb/1.9Gb [15.8s elapsed, 0s remaining, 144.4Mb/s]      \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:eta.core.utils: 100% |██████|    1.9Gb/1.9Gb [15.8s elapsed, 0s remaining, 144.4Mb/s]      \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting annotations to '/root/fiftyone/coco-2017/raw/instances_train2017.json'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.utils.coco:Extracting annotations to '/root/fiftyone/coco-2017/raw/instances_train2017.json'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading 350 images\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.utils.coco:Downloading 350 images\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 100% |██████████████████| 350/350 [3.6m elapsed, 0s remaining, 1.7 images/s]      \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:eta.core.utils: 100% |██████████████████| 350/350 [3.6m elapsed, 0s remaining, 1.7 images/s]      \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing annotations for 350 downloaded samples to '/root/fiftyone/coco-2017/train/labels.json'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.utils.coco:Writing annotations for 350 downloaded samples to '/root/fiftyone/coco-2017/train/labels.json'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading split 'validation' to '/root/fiftyone/coco-2017/validation' if necessary\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.zoo.datasets:Downloading split 'validation' to '/root/fiftyone/coco-2017/validation' if necessary\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found annotations at '/root/fiftyone/coco-2017/raw/instances_val2017.json'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.utils.coco:Found annotations at '/root/fiftyone/coco-2017/raw/instances_val2017.json'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading 350 images\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.utils.coco:Downloading 350 images\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 100% |██████████████████| 350/350 [3.6m elapsed, 0s remaining, 1.6 images/s]      \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:eta.core.utils: 100% |██████████████████| 350/350 [3.6m elapsed, 0s remaining, 1.6 images/s]      \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing annotations for 350 downloaded samples to '/root/fiftyone/coco-2017/validation/labels.json'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.utils.coco:Writing annotations for 350 downloaded samples to '/root/fiftyone/coco-2017/validation/labels.json'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset info written to '/root/fiftyone/coco-2017/info.json'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.zoo.datasets:Dataset info written to '/root/fiftyone/coco-2017/info.json'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading 'coco-2017' split 'train'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.zoo.datasets:Loading 'coco-2017' split 'train'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 100% |█████████████████| 350/350 [1.9s elapsed, 0s remaining, 185.0 samples/s]         \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:eta.core.utils: 100% |█████████████████| 350/350 [1.9s elapsed, 0s remaining, 185.0 samples/s]         \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading 'coco-2017' split 'validation'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.zoo.datasets:Loading 'coco-2017' split 'validation'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 100% |█████████████████| 350/350 [1.9s elapsed, 0s remaining, 184.5 samples/s]      \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:eta.core.utils: 100% |█████████████████| 350/350 [1.9s elapsed, 0s remaining, 184.5 samples/s]      \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset 'coco-2017-train-validation-350' created\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.zoo.datasets:Dataset 'coco-2017-train-validation-350' created\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from pycocotools.coco import COCO\n",
        "import requests\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "\n",
        "# 創建目錄結構\n",
        "os.makedirs(\"data/mini_coco_det/train/images\", exist_ok=True)\n",
        "os.makedirs(\"data/mini_coco_det/val/images\", exist_ok=True)\n",
        "os.makedirs(\"data/mini_coco_det/train/annotations\", exist_ok=True)\n",
        "os.makedirs(\"data/mini_coco_det/val/annotations\", exist_ok=True)\n",
        "\n",
        "# 從 FiftyOne 數據集中提取樣本\n",
        "samples = list(dataset)\n",
        "random.shuffle(samples)\n",
        "\n",
        "# 分割為 train (240) 和 val (60)\n",
        "train_samples = samples[:240]\n",
        "val_samples = samples[240:300]\n",
        "\n",
        "def download_and_process_samples(samples, split_name, target_count):\n",
        "    \"\"\"下載並處理樣本\"\"\"\n",
        "    images_info = []\n",
        "    annotations_info = []\n",
        "\n",
        "    for idx, sample in enumerate(samples[:target_count]):\n",
        "        # 獲取圖片信息\n",
        "        image_path = sample.filepath\n",
        "        image_name = os.path.basename(image_path)\n",
        "\n",
        "        # 複製圖片到目標目錄\n",
        "        target_path = f\"data/mini_coco_det/{split_name}/images/{image_name}\"\n",
        "        shutil.copy2(image_path, target_path)\n",
        "\n",
        "        # 獲取圖片尺寸\n",
        "        from PIL import Image\n",
        "        with Image.open(target_path) as img:\n",
        "            width, height = img.size\n",
        "\n",
        "        # 構建圖片信息\n",
        "        image_info = {\n",
        "            \"id\": idx + 1,\n",
        "            \"width\": width,\n",
        "            \"height\": height,\n",
        "            \"file_name\": image_name\n",
        "        }\n",
        "        images_info.append(image_info)\n",
        "\n",
        "        # 處理標註信息\n",
        "        if sample.ground_truth and sample.ground_truth.detections:\n",
        "            for det_idx, detection in enumerate(sample.ground_truth.detections):\n",
        "                # 獲取邊界框座標 (相對座標轉絕對座標)\n",
        "                bbox = detection.bounding_box\n",
        "                x = bbox[0] * width\n",
        "                y = bbox[1] * height\n",
        "                w = bbox[2] * width\n",
        "                h = bbox[3] * height\n",
        "\n",
        "                # 獲取類別 ID\n",
        "                class_name = detection.label\n",
        "                if class_name in selected_classes:\n",
        "                    class_id = selected_classes.index(class_name)\n",
        "\n",
        "                    annotation = {\n",
        "                        \"id\": len(annotations_info) + 1,\n",
        "                        \"image_id\": idx + 1,\n",
        "                        \"category_id\": class_id,\n",
        "                        \"bbox\": [x, y, w, h],\n",
        "                        \"area\": w * h,\n",
        "                        \"iscrowd\": getattr(detection, 'iscrowd', 0)\n",
        "                    }\n",
        "                    annotations_info.append(annotation)\n",
        "\n",
        "    return images_info, annotations_info\n",
        "\n",
        "# 處理訓練集\n",
        "print(\"處理訓練集...\")\n",
        "train_images, train_annotations = download_and_process_samples(train_samples, \"train\", 240)\n",
        "\n",
        "# 處理驗證集\n",
        "print(\"處理驗證集...\")\n",
        "val_images, val_annotations = download_and_process_samples(val_samples, \"val\", 60)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cgQxtIDNKo2c",
        "outputId": "68154de0-e5da-434d-b71c-bc467962516a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "處理訓練集...\n",
            "處理驗證集...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 創建類別信息\n",
        "categories = []\n",
        "for idx, class_name in enumerate(selected_classes):\n",
        "    categories.append({\n",
        "        \"id\": idx + 1,\n",
        "        \"name\": class_name,\n",
        "        \"supercategory\": \"object\"\n",
        "    })\n",
        "\n",
        "# 創建訓練集標註文件\n",
        "train_annotation = {\n",
        "    \"info\": {\n",
        "        \"description\": \"Custom COCO Dataset - Train Split\",\n",
        "        \"version\": \"1.0\",\n",
        "        \"year\": 2025\n",
        "    },\n",
        "    \"licenses\": [],\n",
        "    \"images\": train_images,\n",
        "    \"annotations\": train_annotations,\n",
        "    \"categories\": categories\n",
        "}\n",
        "\n",
        "# 創建驗證集標註文件\n",
        "val_annotation = {\n",
        "    \"info\": {\n",
        "        \"description\": \"Custom COCO Dataset - Validation Split\",\n",
        "        \"version\": \"1.0\",\n",
        "        \"year\": 2025\n",
        "    },\n",
        "    \"licenses\": [],\n",
        "    \"images\": val_images,\n",
        "    \"annotations\": val_annotations,\n",
        "    \"categories\": categories\n",
        "}\n",
        "\n",
        "# 保存標註文件\n",
        "with open(\"data/mini_coco_det/train/annotations/annotations.json\", \"w\") as f:\n",
        "    json.dump(train_annotation, f, indent=2)\n",
        "\n",
        "with open(\"data/mini_coco_det/val/annotations/annotations.json\", \"w\") as f:\n",
        "    json.dump(val_annotation, f, indent=2)\n",
        "\n",
        "print(f\"數據集創建完成！\")\n",
        "print(f\"訓練集: {len(train_images)} 張圖片, {len(train_annotations)} 個標註\")\n",
        "print(f\"驗證集: {len(val_images)} 張圖片, {len(val_annotations)} 個標註\")\n",
        "print(f\"類別數量: {len(categories)}\")\n",
        "print(f\"選擇的類別: {selected_classes}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ZgcBz2xKvge",
        "outputId": "dc407998-f67a-483a-db4a-bd122691d339"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "數據集創建完成！\n",
            "訓練集: 240 張圖片, 1040 個標註\n",
            "驗證集: 60 張圖片, 368 個標註\n",
            "類別數量: 10\n",
            "選擇的類別: ['person', 'car', 'bicycle', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'dog']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 檢查文件結構\n",
        "\n",
        "# 統計每個分割的圖片數量\n",
        "train_count = len(os.listdir(\"data/mini_coco_det/train/images\"))\n",
        "val_count = len(os.listdir(\"data/mini_coco_det/val/images\"))\n",
        "\n",
        "print(f\"訓練集圖片數量: {train_count}\")\n",
        "print(f\"驗證集圖片數量: {val_count}\")\n",
        "\n",
        "# 檢查標註文件\n",
        "with open(\"data/mini_coco_det/train/annotations/annotations.json\", \"r\") as f:\n",
        "    train_data = json.load(f)\n",
        "\n",
        "print(f\"訓練集標註統計:\")\n",
        "print(f\"  圖片: {len(train_data['images'])}\")\n",
        "print(f\"  標註: {len(train_data['annotations'])}\")\n",
        "print(f\"  類別: {len(train_data['categories'])}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ox208RhaKxZ6",
        "outputId": "e282109e-414b-4008-a3e6-66cd09dc0a8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "訓練集圖片數量: 240\n",
            "驗證集圖片數量: 60\n",
            "訓練集標註統計:\n",
            "  圖片: 240\n",
            "  標註: 1040\n",
            "  類別: 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PASCAL VOC 2012"
      ],
      "metadata": {
        "id": "_jbL5YEyQo4F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((512, 512)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# 下載 PASCAL VOC 2012\n",
        "print(\"下載資料集...\")\n",
        "voc_dataset = datasets.VOCDetection(\n",
        "    root='./data/mini_voc_seg/',\n",
        "    year='2012',\n",
        "    image_set='train',\n",
        "    download=True,\n",
        "    transform=None\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ySEqsxYQ0uF",
        "outputId": "22b08239-db73-4de0-cdf8-9f976511d036"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "下載資料集...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2.00G/2.00G [01:43<00:00, 19.2MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import random\n",
        "from PIL import Image\n",
        "\n",
        "def convert_voc_to_minivoc_seg():\n",
        "    \"\"\"將下載的 VOC2012 轉換為作業要求的 Mini-VOC-Seg 格式\"\"\"\n",
        "\n",
        "    # VOC2012 路徑\n",
        "    voc_root = \"./data/mini_voc_seg/VOCdevkit/VOC2012\"  # 你的 VOC2012 目錄\n",
        "    images_dir = os.path.join(voc_root, \"JPEGImages\")\n",
        "    seg_class_dir = os.path.join(voc_root, \"SegmentationClass\")\n",
        "\n",
        "    # 獲取所有有分割標註的圖片\n",
        "    seg_images = []\n",
        "    for img_file in os.listdir(seg_class_dir):\n",
        "        if img_file.endswith('.png'):\n",
        "            img_id = img_file.replace('.png', '')\n",
        "            jpg_path = os.path.join(images_dir, f\"{img_id}.jpg\")\n",
        "            png_path = os.path.join(seg_class_dir, img_file)\n",
        "\n",
        "            if os.path.exists(jpg_path):\n",
        "                seg_images.append((jpg_path, png_path, img_id))\n",
        "\n",
        "    print(f\"找到 {len(seg_images)} 張有分割標註的圖片\")\n",
        "\n",
        "    # 隨機 shuffle 並選擇 300 張\n",
        "    random.seed(42)\n",
        "    random.shuffle(seg_images)\n",
        "    selected_images = seg_images[:300]\n",
        "\n",
        "    # 分割為 240 train / 60 val\n",
        "    train_images = selected_images[:240]\n",
        "    val_images = selected_images[240:300]\n",
        "\n",
        "    # 創建作業要求的目錄結構\n",
        "    os.makedirs(\"data/mini_voc_seg/train/images\", exist_ok=True)\n",
        "    os.makedirs(\"data/mini_voc_seg/val/images\", exist_ok=True)\n",
        "    os.makedirs(\"data/mini_voc_seg/train/annotations\", exist_ok=True)\n",
        "    os.makedirs(\"data/mini_voc_seg/val/annotations\", exist_ok=True)\n",
        "\n",
        "    def copy_split_data(image_list, split_name):\n",
        "        \"\"\"複製並重命名文件到目標目錄\"\"\"\n",
        "        for idx, (jpg_path, png_path, img_id) in enumerate(image_list):\n",
        "            # 新的文件名格式\n",
        "            new_jpg_name = f\"{split_name}_{idx:06d}.jpg\"\n",
        "            new_png_name = f\"{split_name}_{idx:06d}.png\"\n",
        "\n",
        "            # 目標路徑\n",
        "            dst_jpg = os.path.join(f\"data/mini_voc_seg/{split_name}/images\", new_jpg_name)\n",
        "            dst_png = os.path.join(f\"data/mini_voc_seg/{split_name}/annotations\", new_png_name)\n",
        "\n",
        "            # 複製文件\n",
        "            shutil.copy2(jpg_path, dst_jpg)\n",
        "            shutil.copy2(png_path, dst_png)\n",
        "\n",
        "            if (idx + 1) % 50 == 0:\n",
        "                print(f\"已處理 {split_name} 集 {idx + 1} 張圖片\")\n",
        "\n",
        "    # 處理訓練集和驗證集\n",
        "    print(\"處理訓練集...\")\n",
        "    copy_split_data(train_images, \"train\")\n",
        "\n",
        "    print(\"處理驗證集...\")\n",
        "    copy_split_data(val_images, \"val\")\n",
        "\n",
        "    print(f\"\\nMini-VOC-Seg 數據集創建完成！\")\n",
        "    print(f\"訓練集: {len(train_images)} 張\")\n",
        "    print(f\"驗證集: {len(val_images)} 張\")\n",
        "\n",
        "    return len(train_images), len(val_images)\n",
        "\n",
        "# 執行轉換\n",
        "train_count, val_count = convert_voc_to_minivoc_seg()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rR8iJe5emJeZ",
        "outputId": "5af53f32-3a39-4c3f-954f-6a8a3a46ed09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "找到 2913 張有分割標註的圖片\n",
            "處理訓練集...\n",
            "已處理 train 集 50 張圖片\n",
            "已處理 train 集 100 張圖片\n",
            "已處理 train 集 150 張圖片\n",
            "已處理 train 集 200 張圖片\n",
            "處理驗證集...\n",
            "已處理 val 集 50 張圖片\n",
            "\n",
            "Mini-VOC-Seg 數據集創建完成！\n",
            "訓練集: 240 張\n",
            "驗證集: 60 張\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imagemette-160"
      ],
      "metadata": {
        "id": "9jgasRy6qW0b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "import os\n",
        "import shutil\n",
        "import random\n",
        "import json\n",
        "from collections import defaultdict\n",
        "\n",
        "def download_imagenette_160():\n",
        "    \"\"\"使用 PyTorch API 下載 Imagenette-160\"\"\"\n",
        "\n",
        "    # 下載訓練集\n",
        "    train_dataset = datasets.Imagenette(\n",
        "        root='./data/imagenette_data',\n",
        "        split='train',\n",
        "        size='160px',\n",
        "        download=True,\n",
        "        transform=None\n",
        "    )\n",
        "\n",
        "    # 下載驗證集\n",
        "    val_dataset = datasets.Imagenette(\n",
        "        root='./data/imagenette_160',\n",
        "        split='val',\n",
        "        size='160px',\n",
        "        download=True,\n",
        "        transform=None\n",
        "    )\n",
        "\n",
        "    print(f\"Imagenette-160 訓練集: {len(train_dataset)} 張\")\n",
        "    print(f\"Imagenette-160 驗證集: {len(val_dataset)} 張\")\n",
        "    print(f\"類別數: {len(train_dataset.classes)}\")\n",
        "\n",
        "    return train_dataset, val_dataset\n",
        "\n",
        "# 執行下載\n",
        "train_dataset, val_dataset = download_imagenette_160()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bgiksnz_qeHy",
        "outputId": "98409647-47ef-455f-b740-964c61567d43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 99.0M/99.0M [00:19<00:00, 5.03MB/s]\n",
            "100%|██████████| 99.0M/99.0M [00:20<00:00, 4.90MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Imagenette-160 訓練集: 9469 張\n",
            "Imagenette-160 驗證集: 3925 張\n",
            "類別數: 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import random\n",
        "from collections import defaultdict\n",
        "import json\n",
        "\n",
        "def select_imagenette_samples():\n",
        "    \"\"\"從現有的 Imagenette-160 結構中挑選指定數量的圖片\"\"\"\n",
        "\n",
        "    # 設定隨機種子\n",
        "    random.seed(42)\n",
        "\n",
        "    # 正確的路徑（根據你的目錄結構）\n",
        "    train_dir = \"./data/imagenette_160/imagenette2-160/train\"\n",
        "    val_dir = \"./data/imagenette_160/imagenette2-160/val\"\n",
        "\n",
        "    # 檢查路徑是否存在\n",
        "    if not os.path.exists(train_dir):\n",
        "        print(f\"錯誤: 找不到目錄 {train_dir}\")\n",
        "        return\n",
        "    if not os.path.exists(val_dir):\n",
        "        print(f\"錯誤: 找不到目錄 {val_dir}\")\n",
        "        return\n",
        "\n",
        "    # 類別映射\n",
        "    class_mapping = {\n",
        "        'n01440764': 'tench',\n",
        "        'n02102040': 'English_springer',\n",
        "        'n02979186': 'cassette_player',\n",
        "        'n03000684': 'chain_saw',\n",
        "        'n03028079': 'church',\n",
        "        'n03394916': 'French_horn',\n",
        "        'n03417042': 'garbage_truck',\n",
        "        'n03425413': 'gas_pump',\n",
        "        'n03445777': 'golf_ball',\n",
        "        'n03888257': 'parachute'\n",
        "    }\n",
        "\n",
        "    # 收集每個類別的圖片\n",
        "    def collect_class_images(base_dir):\n",
        "        class_images = defaultdict(list)\n",
        "        for class_folder in os.listdir(base_dir):\n",
        "            class_path = os.path.join(base_dir, class_folder)\n",
        "            if os.path.isdir(class_path) and class_folder in class_mapping:\n",
        "                for img_file in os.listdir(class_path):\n",
        "                    if img_file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "                        img_path = os.path.join(class_path, img_file)\n",
        "                        class_images[class_folder].append(img_path)\n",
        "        return class_images\n",
        "\n",
        "    # 收集訓練集和驗證集圖片\n",
        "    print(\"收集訓練集圖片...\")\n",
        "    train_class_images = collect_class_images(train_dir)\n",
        "\n",
        "    print(\"收集驗證集圖片...\")\n",
        "    val_class_images = collect_class_images(val_dir)\n",
        "\n",
        "    # 顯示每個類別的圖片數量\n",
        "    print(\"\\n原始數據統計:\")\n",
        "    for class_id in class_mapping.keys():\n",
        "        train_count = len(train_class_images.get(class_id, []))\n",
        "        val_count = len(val_class_images.get(class_id, []))\n",
        "        print(f\"{class_mapping[class_id]} ({class_id}): train={train_count}, val={val_count}\")\n",
        "\n",
        "    # 從每個類別隨機挑選指定數量的圖片\n",
        "    train_selected = {}\n",
        "    val_selected = {}\n",
        "\n",
        "    for class_id, class_name in class_mapping.items():\n",
        "        # 訓練集：每類挑選 24 張\n",
        "        train_images = train_class_images.get(class_id, [])\n",
        "        if len(train_images) >= 24:\n",
        "            train_selected[class_id] = random.sample(train_images, 24)\n",
        "        else:\n",
        "            train_selected[class_id] = train_images\n",
        "            print(f\"警告: {class_name} 訓練集只有 {len(train_images)} 張圖片，少於 24 張\")\n",
        "\n",
        "        # 驗證集：每類挑選 6 張\n",
        "        val_images = val_class_images.get(class_id, [])\n",
        "        if len(val_images) >= 6:\n",
        "            val_selected[class_id] = random.sample(val_images, 6)\n",
        "        else:\n",
        "            val_selected[class_id] = val_images\n",
        "            print(f\"警告: {class_name} 驗證集只有 {len(val_images)} 張圖片，少於 6 張\")\n",
        "\n",
        "    return train_selected, val_selected, class_mapping\n",
        "\n",
        "# 執行挑選\n",
        "train_selected, val_selected, class_mapping = select_imagenette_samples()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pf5qRqPBtjlT",
        "outputId": "8be91edb-411e-45ed-e393-627070c8072b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "收集訓練集圖片...\n",
            "收集驗證集圖片...\n",
            "\n",
            "原始數據統計:\n",
            "tench (n01440764): train=963, val=387\n",
            "English_springer (n02102040): train=955, val=395\n",
            "cassette_player (n02979186): train=993, val=357\n",
            "chain_saw (n03000684): train=858, val=386\n",
            "church (n03028079): train=941, val=409\n",
            "French_horn (n03394916): train=956, val=394\n",
            "garbage_truck (n03417042): train=961, val=389\n",
            "gas_pump (n03425413): train=931, val=419\n",
            "golf_ball (n03445777): train=951, val=399\n",
            "parachute (n03888257): train=960, val=390\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def save_mini_imagenette_160(train_selected, val_selected, class_mapping):\n",
        "    \"\"\"保存挑選的圖片為作業要求的格式\"\"\"\n",
        "\n",
        "    # 創建目標目錄\n",
        "    os.makedirs(\"data/imagenette_160/train/images\", exist_ok=True)\n",
        "    os.makedirs(\"data/imagenette_160/train/annotations\", exist_ok=True)\n",
        "    os.makedirs(\"data/imagenette_160/val/images\", exist_ok=True)\n",
        "    os.makedirs(\"data/imagenette_160/val/annotations\", exist_ok=True)\n",
        "\n",
        "    def process_split(selected_images, split_name):\n",
        "        \"\"\"處理單個分割的數據\"\"\"\n",
        "        all_samples = []\n",
        "        class_counts = defaultdict(int)\n",
        "\n",
        "        # 將所有選中的圖片合併到一個列表中\n",
        "        for class_id, image_paths in selected_images.items():\n",
        "            class_name = class_mapping[class_id]\n",
        "            for img_path in image_paths:\n",
        "                all_samples.append((img_path, class_name, class_id))\n",
        "                class_counts[class_name] += 1\n",
        "\n",
        "        # 隨機打亂所有樣本\n",
        "        random.shuffle(all_samples)\n",
        "\n",
        "        # 保存圖片並創建標籤\n",
        "        labels_info = []\n",
        "        class_to_idx = {name: idx for idx, name in enumerate(sorted(class_mapping.values()))}\n",
        "\n",
        "        for idx, (img_path, class_name, class_id) in enumerate(all_samples):\n",
        "            # 新文件名\n",
        "            img_filename = f\"{split_name}_{idx:06d}.jpg\"\n",
        "            dst_path = os.path.join(f\"data/imagenette_160/{split_name}/images\", img_filename)\n",
        "\n",
        "            # 複製圖片\n",
        "            shutil.copy2(img_path, dst_path)\n",
        "\n",
        "            # 記錄標籤信息\n",
        "            label_info = {\n",
        "                \"filename\": img_filename,\n",
        "                \"class_name\": class_name,\n",
        "                \"class_id\": class_to_idx[class_name],\n",
        "                \"original_class_id\": class_id,\n",
        "                \"original_path\": img_path\n",
        "            }\n",
        "            labels_info.append(label_info)\n",
        "\n",
        "            if (idx + 1) % 30 == 0:\n",
        "                print(f\"已處理 {split_name} 集 {idx + 1} 張圖片\")\n",
        "\n",
        "        # 保存標籤文件\n",
        "        labels_data = {\n",
        "            \"split\": split_name,\n",
        "            \"num_images\": len(all_samples),\n",
        "            \"classes\": sorted(class_mapping.values()),\n",
        "            \"class_to_idx\": class_to_idx,\n",
        "            \"class_distribution\": dict(class_counts),\n",
        "            \"labels\": labels_info\n",
        "        }\n",
        "\n",
        "        with open(f\"data/imagenette_160/{split_name}/annotations/labels.json\", \"w\") as f:\n",
        "            json.dump(labels_data, f, indent=2)\n",
        "\n",
        "        return len(all_samples), class_counts\n",
        "\n",
        "    # 處理訓練集\n",
        "    print(\"\\n處理訓練集...\")\n",
        "    train_count, train_class_counts = process_split(train_selected, \"train\")\n",
        "\n",
        "    # 處理驗證集\n",
        "    print(\"處理驗證集...\")\n",
        "    val_count, val_class_counts = process_split(val_selected, \"val\")\n",
        "\n",
        "    # 創建數據集信息\n",
        "    dataset_info = {\n",
        "        \"name\": \"Imagenette_160\",\n",
        "        \"description\": \"Selected subset from Imagenette-160 for multi-task learning\",\n",
        "        \"total_images\": train_count + val_count,\n",
        "        \"train_images\": train_count,\n",
        "        \"val_images\": val_count,\n",
        "        \"num_classes\": len(class_mapping),\n",
        "        \"classes\": sorted(class_mapping.values()),\n",
        "        \"selection_strategy\": \"24 per class for train, 6 per class for val\",\n",
        "        \"train_class_distribution\": dict(train_class_counts),\n",
        "        \"val_class_distribution\": dict(val_class_counts)\n",
        "    }\n",
        "\n",
        "    with open(\"data/imagenette_160/dataset_info.json\", \"w\") as f:\n",
        "        json.dump(dataset_info, f, indent=2)\n",
        "\n",
        "    print(f\"\\nMini-Imagenette-160 創建完成！\")\n",
        "    print(f\"訓練集: {train_count} 張圖片\")\n",
        "    print(f\"驗證集: {val_count} 張圖片\")\n",
        "\n",
        "    print(\"\\n訓練集類別分佈:\")\n",
        "    for class_name, count in sorted(train_class_counts.items()):\n",
        "        print(f\"  {class_name}: {count} 張\")\n",
        "\n",
        "    print(\"\\n驗證集類別分佈:\")\n",
        "    for class_name, count in sorted(val_class_counts.items()):\n",
        "        print(f\"  {class_name}: {count} 張\")\n",
        "\n",
        "# 執行保存\n",
        "save_mini_imagenette_160(train_selected, val_selected, class_mapping)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BV40u8RgtuUx",
        "outputId": "86eaff07-d75d-4671-d9a1-90cce5ca1b93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "處理訓練集...\n",
            "已處理 train 集 30 張圖片\n",
            "已處理 train 集 60 張圖片\n",
            "已處理 train 集 90 張圖片\n",
            "已處理 train 集 120 張圖片\n",
            "已處理 train 集 150 張圖片\n",
            "已處理 train 集 180 張圖片\n",
            "已處理 train 集 210 張圖片\n",
            "已處理 train 集 240 張圖片\n",
            "處理驗證集...\n",
            "已處理 val 集 30 張圖片\n",
            "已處理 val 集 60 張圖片\n",
            "\n",
            "Mini-Imagenette-160 創建完成！\n",
            "訓練集: 240 張圖片\n",
            "驗證集: 60 張圖片\n",
            "\n",
            "訓練集類別分佈:\n",
            "  English_springer: 24 張\n",
            "  French_horn: 24 張\n",
            "  cassette_player: 24 張\n",
            "  chain_saw: 24 張\n",
            "  church: 24 張\n",
            "  garbage_truck: 24 張\n",
            "  gas_pump: 24 張\n",
            "  golf_ball: 24 張\n",
            "  parachute: 24 張\n",
            "  tench: 24 張\n",
            "\n",
            "驗證集類別分佈:\n",
            "  English_springer: 6 張\n",
            "  French_horn: 6 張\n",
            "  cassette_player: 6 張\n",
            "  chain_saw: 6 張\n",
            "  church: 6 張\n",
            "  garbage_truck: 6 張\n",
            "  gas_pump: 6 張\n",
            "  golf_ball: 6 張\n",
            "  parachute: 6 張\n",
            "  tench: 6 張\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def verify_final_dataset():\n",
        "    \"\"\"驗證最終的數據集\"\"\"\n",
        "\n",
        "    print(\"驗證 Mini-Imagenette-160 數據集:\")\n",
        "\n",
        "    total_size = 0\n",
        "    for split in ['train', 'val']:\n",
        "        split_dir = f\"data/imagenette-160/{split}\"\n",
        "\n",
        "        if os.path.exists(split_dir):\n",
        "            # 統計圖片文件\n",
        "            image_files = [f for f in os.listdir(split_dir) if f.endswith('.jpg')]\n",
        "\n",
        "            # 計算大小\n",
        "            split_size = 0\n",
        "            for file in os.listdir(split_dir):\n",
        "                file_path = os.path.join(split_dir, file)\n",
        "                split_size += os.path.getsize(file_path)\n",
        "\n",
        "            total_size += split_size\n",
        "\n",
        "            print(f\"\\n{split} 集:\")\n",
        "            print(f\"  圖片數量: {len(image_files)}\")\n",
        "            print(f\"  大小: {split_size / (1024*1024):.1f} MB\")\n",
        "\n",
        "            # 檢查標籤文件\n",
        "            labels_file = os.path.join(split_dir, \"labels.json\")\n",
        "            if os.path.exists(labels_file):\n",
        "                with open(labels_file, 'r') as f:\n",
        "                    labels_data = json.load(f)\n",
        "                print(f\"  類別數: {len(labels_data['classes'])}\")\n",
        "                print(f\"  每類分佈: {labels_data['class_distribution']}\")\n",
        "\n",
        "    print(f\"\\n總大小: {total_size / (1024*1024):.1f} MB\")\n",
        "    print(f\"符合作業要求: ≈25 MB, 240 train + 60 val\")\n",
        "\n",
        "verify_final_dataset()"
      ],
      "metadata": {
        "id": "JVDJl4oXtyUy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "048a066d-ed63-47ba-b0e6-081be6058076"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "驗證 Mini-Imagenette-160 數據集:\n",
            "\n",
            "總大小: 0.0 MB\n",
            "符合作業要求: ≈25 MB, 240 train + 60 val\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "7HhqjW5uPQsv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1A8zUSsNPTxp"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision.models import mobilenet_v3_small\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "class MultiTaskHead(nn.Module):\n",
        "    def __init__(self, in_channels, num_det_classes=10, num_seg_classes=21, num_cls_classes=10):\n",
        "        super().__init__()\n",
        "\n",
        "        # 共享特徵提取層 (符合參數限制)\n",
        "        self.shared_conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 256, 3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 128, 3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        # 分割專用上採樣路徑 (新增部分)\n",
        "        self.seg_upsample = nn.Sequential(\n",
        "            nn.Upsample(scale_factor=32, mode='bilinear', align_corners=True),  # 關鍵修正\n",
        "            nn.Conv2d(128, num_seg_classes, 1)  # 輸出通道數=類別數\n",
        "        )\n",
        "\n",
        "        # 檢測與分類輸出層\n",
        "        self.det_cls_conv = nn.Conv2d(128, 6 + num_det_classes + num_cls_classes, 1)\n",
        "\n",
        "        self.num_det_classes = num_det_classes\n",
        "        self.num_cls_classes = num_cls_classes\n",
        "\n",
        "    def forward(self, x):\n",
        "        shared_feat = self.shared_conv(x)\n",
        "\n",
        "        # 分割輸出 (512x512)\n",
        "        seg_output = self.seg_upsample(shared_feat)  # [B,21,512,512]\n",
        "\n",
        "        # 檢測與分類輸出\n",
        "        det_cls_output = self.det_cls_conv(shared_feat)\n",
        "        det_output = det_cls_output[:, :6+self.num_det_classes]\n",
        "        cls_output = F.adaptive_avg_pool2d(\n",
        "            det_cls_output[:, 6+self.num_det_classes:], (1, 1)\n",
        "        ).squeeze(-1).squeeze(-1)\n",
        "\n",
        "        return det_output, seg_output, cls_output\n",
        "\n",
        "\n",
        "class MultiTaskModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MultiTaskModel, self).__init__()\n",
        "\n",
        "        # 使用 MobileNetV3-Small 作為骨幹網路\n",
        "        self.backbone = mobilenet_v3_small(pretrained=True)\n",
        "        self.backbone.classifier = nn.Identity()  # 移除分類頭\n",
        "\n",
        "        # 簡單的頸部網路\n",
        "        self.neck = nn.Sequential(\n",
        "            nn.Conv2d(576, 256, 3, padding=1),  # MobileNetV3-Small 輸出通道數\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        # 多任務頭部\n",
        "        self.head = MultiTaskHead(256)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 骨幹網路特徵提取\n",
        "        features = self.backbone.features(x)\n",
        "\n",
        "        # 頸部處理\n",
        "        neck_feat = self.neck(features)\n",
        "\n",
        "        # 多任務輸出\n",
        "        det_out, seg_out, cls_out = self.head(neck_feat)\n",
        "\n",
        "        return det_out, seg_out, cls_out\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torchvision.ops import box_iou  # 添加這行導入\n",
        "\n",
        "class EWC:\n",
        "    def __init__(self, model, task_loaders, importance=1000):\n",
        "        \"\"\"\n",
        "        多任務版EWC正確實作\n",
        "        :param task_loaders: 字典格式 {\n",
        "            'seg': seg_loader,\n",
        "            'det': det_loader,\n",
        "            'cls': cls_loader\n",
        "        }\n",
        "        \"\"\"\n",
        "        self.model = model\n",
        "        self.importance = importance\n",
        "        self.params = {n: p.clone().detach() for n, p in model.named_parameters() if p.requires_grad}\n",
        "        self.fisher = self._compute_multitask_fisher(task_loaders)\n",
        "\n",
        "    def _compute_multitask_fisher(self, task_loaders):\n",
        "        fisher = {n: torch.zeros_like(p) for n, p in self.model.named_parameters() if p.requires_grad}\n",
        "\n",
        "        self.model.eval()\n",
        "\n",
        "        # 驗證輸入格式\n",
        "        if not isinstance(task_loaders, dict):\n",
        "            raise TypeError(\"task_loaders必須是字典格式，例如: {'seg': seg_loader, ...}\")\n",
        "\n",
        "        # 分任務計算\n",
        "        for task_name, loader in task_loaders.items():\n",
        "            print(f\"正在計算 {task_name} 任務的Fisher信息...\")\n",
        "            for data, targets in tqdm(loader, desc=task_name):\n",
        "                self.model.zero_grad()\n",
        "                data = data.to(self.model.device)\n",
        "                if task_name == 'det':\n",
        "                    # 手动移动每个目标的键值到设备\n",
        "                    device_targets = []\n",
        "                    for t in targets:\n",
        "                        device_targets.append({\n",
        "                            'boxes': t['boxes'].to(self.model.device),\n",
        "                            'labels': t['labels'].to(self.model.device),\n",
        "                            'image_id': t['image_id'].to(self.model.device)\n",
        "                        })\n",
        "                else:\n",
        "                    # 其他任务直接移动整个张量\n",
        "                    targets = targets.to(self.model.device)\n",
        "\n",
        "                # 根據任務類型計算損失\n",
        "                if task_name == 'seg':\n",
        "                    _, seg_out, _ = self.model(data)\n",
        "                    loss = F.cross_entropy(seg_out, targets, ignore_index=255)\n",
        "                elif task_name == 'det':\n",
        "                    det_out, _, _ = self.model(data)\n",
        "                    loss = compute_detection_loss(det_out, targets, 10)\n",
        "                elif task_name == 'cls':\n",
        "                    _, _, cls_out = self.model(data)\n",
        "                    loss = F.cross_entropy(cls_out, targets)\n",
        "\n",
        "                loss.backward()\n",
        "\n",
        "                # 累積Fisher信息\n",
        "                for n, p in self.model.named_parameters():\n",
        "                   if p.requires_grad:\n",
        "                        if p.grad is not None:\n",
        "                            fisher[n] += p.grad.data ** 2\n",
        "                        else:\n",
        "                            # 可選擇初始化或跳過\n",
        "                            fisher[n] += torch.zeros_like(p)\n",
        "\n",
        "        # 正規化\n",
        "        total_batches = sum(len(loader) for loader in task_loaders.values())\n",
        "        for n in fisher:\n",
        "            fisher[n] /= total_batches\n",
        "\n",
        "        return fisher\n",
        "\n",
        "    def penalty(self, model):\n",
        "        loss = 0\n",
        "        for n, p in model.named_parameters():\n",
        "            if n in self.fisher:\n",
        "                loss += (self.fisher[n] * (p - self.params[n]) ** 2).sum()\n",
        "        return self.importance * loss\n",
        "\n",
        "class MultiTaskTrainer:\n",
        "    def __init__(self, model, device):\n",
        "        self.model = model\n",
        "        self.model.device = device  # 新增device屬性\n",
        "        self.device = device\n",
        "        self.ewc = None\n",
        "        self.task_fishers = {}  # 儲存各任務的EWC參數\n",
        "\n",
        "    def train_stage(self, dataloader, task_type, num_epochs=60, lr=1e-4):\n",
        "        optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)\n",
        "        self.model.train()\n",
        "\n",
        "        for epoch in tqdm(range(num_epochs), desc=f\"Training {task_type}\"):\n",
        "            total_loss = 0\n",
        "            for batch_idx, (data, target) in enumerate(dataloader):\n",
        "                data = data.to(self.device)\n",
        "                if task_type == 'detection':\n",
        "                    # 手动移动每个目标的键值到设备\n",
        "                    device_targets = []\n",
        "                    for t in target:\n",
        "                        device_targets.append({\n",
        "                            'boxes': t['boxes'].to(self.device),\n",
        "                            'labels': t['labels'].to(self.device),\n",
        "                            'image_id': t['image_id'].to(self.device)\n",
        "                        })\n",
        "                else:\n",
        "                    # 其他任务直接移动整个张量\n",
        "                    target = target.to(self.device)\n",
        "\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                det_out, seg_out, cls_out = self.model(data)\n",
        "\n",
        "                # 根據任務類型計算損失\n",
        "                if task_type == 'segmentation':\n",
        "                    # print(torch.unique(target))\n",
        "                    loss = F.cross_entropy(seg_out, target, ignore_index=255)\n",
        "                elif task_type == 'detection':\n",
        "                    loss = compute_detection_loss(det_out, target, 10)\n",
        "                elif task_type == 'classification':\n",
        "                    loss = F.cross_entropy(cls_out, target)\n",
        "\n",
        "                # 加入所有已學任務的EWC正則化\n",
        "                if self.ewc is not None:\n",
        "                    loss += self.ewc.penalty(self.model)\n",
        "\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                total_loss += loss.item()\n",
        "\n",
        "            print(f'{task_type} Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(dataloader):.4f}')\n",
        "\n",
        "    def set_ewc(self, task_loaders):\n",
        "        \"\"\"更新EWC參數，需傳入字典格式的task_loaders\"\"\"\n",
        "        self.ewc = EWC(self.model, task_loaders)\n",
        "\n",
        "def compute_detection_loss(predictions, targets, num_classes):\n",
        "    \"\"\"\n",
        "    改進版檢測損失函數，解決設備不一致問題\n",
        "    \"\"\"\n",
        "    total_loss = 0\n",
        "    batch_size = predictions.shape[0]\n",
        "    device = predictions.device  # 獲取預測張量的設備\n",
        "\n",
        "    for i in range(batch_size):\n",
        "        pred = predictions[i]  # [6+num_classes, H, W]\n",
        "        target = targets[i]\n",
        "\n",
        "        if len(target['boxes']) == 0:\n",
        "            continue\n",
        "\n",
        "        # 解碼預測框時確保設備一致\n",
        "        H, W = pred.shape[1], pred.shape[2]\n",
        "\n",
        "        # 生成網格座標時指定設備\n",
        "        y_grid, x_grid = torch.meshgrid(\n",
        "            torch.arange(H, device=device),  # 關鍵修正：添加device參數\n",
        "            torch.arange(W, device=device),\n",
        "            indexing='ij'\n",
        "        )\n",
        "        grid_centers = torch.stack([x_grid, y_grid], dim=-1).float()\n",
        "\n",
        "        # 座標轉換（保持設備一致）\n",
        "        pred_boxes = pred[:4].permute(1, 2, 0)  # [H, W, 4]\n",
        "        pred_boxes_abs = torch.zeros_like(pred_boxes, device=device)  # 明確指定設備\n",
        "\n",
        "        # 確保所有操作在相同設備\n",
        "        pred_boxes_abs[..., 0] = (grid_centers[..., 0] + pred_boxes[..., 0]) / W\n",
        "        pred_boxes_abs[..., 1] = (grid_centers[..., 1] + pred_boxes[..., 1]) / H\n",
        "        pred_boxes_abs[..., 2] = pred_boxes[..., 2]\n",
        "        pred_boxes_abs[..., 3] = pred_boxes[..., 3]\n",
        "\n",
        "        # 後續計算需確保目標數據也在相同設備\n",
        "        target_boxes = target['boxes'].to(device)\n",
        "        target_labels = target['labels'].to(device)\n",
        "\n",
        "        # 計算IOU矩陣（保持設備一致）\n",
        "        flat_boxes = pred_boxes_abs.view(-1, 4)\n",
        "        ious = box_iou(flat_boxes, target_boxes)\n",
        "\n",
        "        # 匈牙利匹配（需處理設備轉換）\n",
        "        match_indices = hungarian_matching(ious)\n",
        "\n",
        "        # 損失計算（全部在相同設備）\n",
        "        if len(match_indices) > 0:\n",
        "            # 座標損失\n",
        "            matched_pred = flat_boxes[match_indices[:, 0]]\n",
        "            matched_target = target_boxes[match_indices[:, 1]]\n",
        "            box_loss = F.mse_loss(matched_pred, matched_target)\n",
        "\n",
        "            # 置信度損失\n",
        "            pred_conf = pred[4].sigmoid().view(-1)\n",
        "            conf_target = torch.zeros_like(pred_conf, device=device)\n",
        "            conf_target[match_indices[:, 0]] = 1\n",
        "            conf_loss = F.binary_cross_entropy(pred_conf, conf_target)\n",
        "\n",
        "            # 分類損失\n",
        "            pred_cls = pred[6:].permute(1, 2, 0).view(-1, num_classes)\n",
        "            cls_loss = F.cross_entropy(\n",
        "                pred_cls[match_indices[:, 0]],\n",
        "                target_labels[match_indices[:, 1]]\n",
        "            )\n",
        "        else:\n",
        "            box_loss = torch.tensor(0.0, device=device)\n",
        "            conf_loss = F.binary_cross_entropy(\n",
        "                pred[4].sigmoid().view(-1),\n",
        "                torch.zeros_like(pred[4].view(-1), device=device)\n",
        "            )\n",
        "            cls_loss = torch.tensor(0.0, device=device)\n",
        "\n",
        "        total_loss += box_loss + conf_loss + cls_loss\n",
        "\n",
        "    return total_loss / batch_size if batch_size > 0 else torch.tensor(0.0)\n",
        "\n",
        "def hungarian_matching(cost_matrix):\n",
        "    \"\"\"改進的匈牙利匹配函數，處理設備轉換\"\"\"\n",
        "    from scipy.optimize import linear_sum_assignment\n",
        "    cost_matrix_np = (1 - cost_matrix).detach().cpu().numpy()\n",
        "    row_ind, col_ind = linear_sum_assignment(cost_matrix_np)\n",
        "    return torch.stack([\n",
        "        torch.tensor(row_ind, dtype=torch.long).to(cost_matrix.device),\n",
        "        torch.tensor(col_ind, dtype=torch.long).to(cost_matrix.device)\n",
        "    ], dim=1)\n"
      ],
      "metadata": {
        "id": "gwCD3T0OQn1d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import json\n",
        "import torchvision.transforms as transforms\n",
        "from pycocotools.coco import COCO\n",
        "import numpy as np\n",
        "\n",
        "class MultiTaskDataset(Dataset):\n",
        "    def __init__(self, data_root, task_type, split='train', transform=None):\n",
        "        self.data_root = data_root\n",
        "        self.task_type = task_type\n",
        "        self.split = split\n",
        "        self.transform = transform\n",
        "        self.samples = []\n",
        "\n",
        "        # 根據任務類型載入對應的數據\n",
        "        if task_type == 'segmentation':\n",
        "            self.data_dir = os.path.join(data_root, 'mini_voc_seg', split)\n",
        "            self._load_segmentation_samples()\n",
        "        elif task_type == 'detection':\n",
        "            self.data_dir = os.path.join(data_root, 'mini_coco_det', split)\n",
        "            self._load_detection_samples()\n",
        "        elif task_type == 'classification':\n",
        "            self.data_dir = os.path.join(data_root, 'imagenette_160', split)\n",
        "            self._load_classification_samples()\n",
        "        else:\n",
        "            raise ValueError(f\"不支援的任務類型: {task_type}\")\n",
        "\n",
        "    def _load_segmentation_samples(self):\n",
        "        \"\"\"載入分割任務的樣本 (VOC格式)\"\"\"\n",
        "        if not os.path.exists(self.data_dir):\n",
        "            raise FileNotFoundError(f\"找不到目錄: {self.data_dir}\")\n",
        "\n",
        "        # 獲取所有 .jpg 圖片文件\n",
        "        for filename in os.listdir(os.path.join(self.data_dir, \"images\")):\n",
        "            if filename.endswith('.jpg'):\n",
        "                img_path = os.path.join(self.data_dir, \"images\", filename)\n",
        "                # 對應的 mask 文件 (.png)\n",
        "                mask_filename = filename.replace('.jpg', '.png')\n",
        "                mask_path = os.path.join(self.data_dir, \"annotations\", mask_filename)\n",
        "                # print(mask_path)\n",
        "\n",
        "                if os.path.exists(mask_path):\n",
        "                    self.samples.append({\n",
        "                        'image_path': img_path,\n",
        "                        'mask_path': mask_path,\n",
        "                        'filename': filename\n",
        "                    })\n",
        "\n",
        "        print(f\"載入分割樣本: {len(self.samples)} 個\")\n",
        "\n",
        "    def _load_detection_samples(self):\n",
        "        \"\"\"載入檢測任務的樣本 (COCO格式)\"\"\"\n",
        "        if not os.path.exists(self.data_dir):\n",
        "            raise FileNotFoundError(f\"找不到目錄: {self.data_dir}\")\n",
        "\n",
        "        # 載入 COCO 格式的標註文件\n",
        "        annotation_file = os.path.join(self.data_dir, \"annotations\", 'annotations.json')\n",
        "        if not os.path.exists(annotation_file):\n",
        "            raise FileNotFoundError(f\"找不到標註文件: {annotation_file}\")\n",
        "\n",
        "        self.coco = COCO(annotation_file)\n",
        "        self.image_ids = list(self.coco.imgs.keys())\n",
        "\n",
        "        for img_id in self.image_ids:\n",
        "            img_info = self.coco.imgs[img_id]\n",
        "            img_path = os.path.join(self.data_dir, \"images\", img_info['file_name'])\n",
        "\n",
        "            # 獲取該圖片的所有標註\n",
        "            ann_ids = self.coco.getAnnIds(imgIds=img_id)\n",
        "            annotations = self.coco.loadAnns(ann_ids)\n",
        "\n",
        "            self.samples.append({\n",
        "                'image_path': img_path,\n",
        "                'image_id': img_id,\n",
        "                'annotations': annotations,\n",
        "                'image_info': img_info\n",
        "            })\n",
        "\n",
        "        print(f\"載入檢測樣本: {len(self.samples)} 個\")\n",
        "\n",
        "    def _load_classification_samples(self):\n",
        "        \"\"\"載入分類任務的樣本 (Imagenette格式)\"\"\"\n",
        "        if not os.path.exists(self.data_dir):\n",
        "            raise FileNotFoundError(f\"找不到目錄: {self.data_dir}\")\n",
        "\n",
        "        # 載入標籤文件\n",
        "        labels_file = os.path.join(self.data_dir, \"annotations\", 'labels.json')\n",
        "        if not os.path.exists(labels_file):\n",
        "            raise FileNotFoundError(f\"找不到標籤文件: {labels_file}\")\n",
        "\n",
        "        with open(labels_file, 'r') as f:\n",
        "            labels_data = json.load(f)\n",
        "\n",
        "        self.classes = labels_data['classes']\n",
        "        self.class_to_idx = labels_data['class_to_idx']\n",
        "\n",
        "        for label_info in labels_data['labels']:\n",
        "            img_path = os.path.join(self.data_dir, \"images\", label_info['filename'])\n",
        "\n",
        "            self.samples.append({\n",
        "                'image_path': img_path,\n",
        "                'class_name': label_info['class_name'],\n",
        "                'class_id': label_info['class_id'],\n",
        "                'filename': label_info['filename']\n",
        "            })\n",
        "\n",
        "        print(f\"載入分類樣本: {len(self.samples)} 個\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.samples[idx]\n",
        "\n",
        "        # 載入圖片\n",
        "        image = Image.open(sample['image_path']).convert('RGB')\n",
        "\n",
        "        if self.task_type == 'segmentation':\n",
        "            # 分割任務：返回圖片和分割 mask\n",
        "            mask = Image.open(sample['mask_path'])\n",
        "\n",
        "            if self.transform:\n",
        "                image = self.transform(image)\n",
        "                # mask 需要特殊處理，不能使用 normalize\n",
        "                mask = mask.resize((512, 512), Image.NEAREST)\n",
        "                mask = torch.from_numpy(np.array(mask)).long()  # int64 (long)\n",
        "                # print(\"get item\",torch.unique(mask))\n",
        "            else:\n",
        "                mask = np.array(mask)\n",
        "            return image, mask\n",
        "\n",
        "        elif self.task_type == 'detection':\n",
        "            # 檢測任務：返回圖片和邊界框標註\n",
        "            annotations = sample['annotations']\n",
        "\n",
        "            # 處理邊界框和標籤\n",
        "            boxes = []\n",
        "            labels = []\n",
        "\n",
        "            for ann in annotations:\n",
        "                # COCO格式: [x, y, width, height]\n",
        "                bbox = ann['bbox']\n",
        "                # 轉換為 [x1, y1, x2, y2] 格式\n",
        "                x1, y1, w, h = bbox\n",
        "                x2, y2 = x1 + w, y1 + h\n",
        "                boxes.append([x1, y1, x2, y2])\n",
        "                labels.append(ann['category_id'])\n",
        "\n",
        "            # 套用轉換 (需同步處理影像和邊界框)\n",
        "            if self.transform:\n",
        "                # 獲取原始影像尺寸\n",
        "                orig_w, orig_h = image.size\n",
        "\n",
        "                # 套用影像轉換\n",
        "                image = self.transform(image)\n",
        "\n",
        "                # 計算縮放比例 (假設transform包含Resize到固定尺寸)\n",
        "                new_h, new_w = image.shape[1], image.shape[2]  # C,H,W格式\n",
        "\n",
        "                # 調整邊界框座標\n",
        "                scale_x = new_w / orig_w\n",
        "                scale_y = new_h / orig_h\n",
        "                boxes = torch.tensor(boxes, dtype=torch.float32)\n",
        "                if len(boxes) > 0:\n",
        "                    boxes[:, [0, 2]] *= scale_x\n",
        "                    boxes[:, [1, 3]] *= scale_y\n",
        "\n",
        "            # 處理空標註情況\n",
        "            if len(boxes) == 0:\n",
        "                boxes = torch.zeros((0, 4), dtype=torch.float32)\n",
        "                labels = torch.zeros((0,), dtype=torch.int64)\n",
        "            else:\n",
        "                boxes = torch.tensor(boxes, dtype=torch.float32)\n",
        "                labels = torch.tensor(labels, dtype=torch.int64)\n",
        "\n",
        "            target = {\n",
        "                'boxes': boxes,\n",
        "                'labels': labels,\n",
        "                'image_id': torch.tensor([sample['image_id']], dtype=torch.int64)\n",
        "            }\n",
        "\n",
        "            return image, target\n",
        "\n",
        "        elif self.task_type == 'classification':\n",
        "            # 分類任務：返回圖片和類別標籤\n",
        "            label = sample['class_id']\n",
        "\n",
        "            if self.transform:\n",
        "                image = self.transform(image)\n",
        "\n",
        "            return image, torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "# 不同任務的資料轉換\n",
        "def get_transforms(task_type, split='train'):\n",
        "    \"\"\"根據任務類型獲取對應的資料轉換\"\"\"\n",
        "\n",
        "    if task_type == 'classification':\n",
        "        if split == 'train':\n",
        "            return transforms.Compose([\n",
        "                transforms.Resize((512, 512)),\n",
        "                transforms.RandomHorizontalFlip(p=0.5),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "            ])\n",
        "        else:\n",
        "            return transforms.Compose([\n",
        "                transforms.Resize((512, 512)),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "            ])\n",
        "\n",
        "    elif task_type == 'segmentation':\n",
        "        return transforms.Compose([\n",
        "            transforms.Resize((512, 512)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "    elif task_type == 'detection':\n",
        "        return transforms.Compose([\n",
        "            transforms.Resize((512, 512)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ])\n"
      ],
      "metadata": {
        "id": "PDCAmPhkQqhO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_segmentation(model, dataloader, device):\n",
        "    model.eval()\n",
        "    total_iou = 0\n",
        "    num_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, target in dataloader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            _, seg_out, _ = model(data)\n",
        "\n",
        "            pred = torch.argmax(seg_out, dim=1)\n",
        "            iou = compute_iou(pred, target)\n",
        "            total_iou += iou\n",
        "            num_samples += 1\n",
        "\n",
        "    return total_iou / num_samples\n",
        "\n",
        "\n",
        "\n",
        "def compute_iou(pred, target):\n",
        "    # 計算IoU的實作\n",
        "    intersection = (pred & target).float().sum()\n",
        "    union = (pred | target).float().sum()\n",
        "    return intersection / union if union > 0 else 0.0\n"
      ],
      "metadata": {
        "id": "bhYuFB3_Qx4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "\n",
        "def evaluate_detection(model, dataloader, device, iou_threshold=0.5):\n",
        "    \"\"\"\n",
        "    評估檢測任務的mAP指標\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    all_predictions = []\n",
        "    all_targets = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, targets in dataloader:\n",
        "            data = data.to(device)\n",
        "\n",
        "            # 模型輸出\n",
        "            det_out, _, _ = model(data)\n",
        "\n",
        "            # 處理檢測輸出 (假設輸出格式為 N x (6 + num_classes))\n",
        "            batch_size = det_out.shape[0]\n",
        "\n",
        "            for i in range(batch_size):\n",
        "                # 解析檢測輸出\n",
        "                pred_boxes, pred_scores, pred_labels = parse_detection_output(det_out[i])\n",
        "\n",
        "                all_predictions.append({\n",
        "                    'boxes': pred_boxes,\n",
        "                    'scores': pred_scores,\n",
        "                    'labels': pred_labels\n",
        "                })\n",
        "\n",
        "                # 處理ground truth\n",
        "                target = targets[i] if isinstance(targets, list) else targets\n",
        "                all_targets.append(target)\n",
        "\n",
        "    # 計算mAP\n",
        "    mAP = compute_map(all_predictions, all_targets, iou_threshold)\n",
        "    return mAP\n",
        "\n",
        "def parse_detection_output(det_output, conf_threshold=0.5, num_classes=10):\n",
        "    \"\"\"\n",
        "    解析检测输出（假设输出形状为 [16, H, W]）\n",
        "    det_output: 模型输出的检测头结果，形状 [16, H, W]\n",
        "    \"\"\"\n",
        "    # 分离参数\n",
        "    cx = det_output[0]   # 中心x坐标 [H, W]\n",
        "    cy = det_output[1]   # 中心y坐标 [H, W]\n",
        "    w = det_output[2]    # 宽度 [H, W]\n",
        "    h = det_output[3]    # 高度 [H, W]\n",
        "    conf = det_output[4].sigmoid()  # 置信度 [H, W]\n",
        "    cls_probs = det_output[5:5+num_classes].softmax(dim=0)  # 类别概率 [num_classes, H, W]\n",
        "\n",
        "    # 生成网格坐标\n",
        "    grid_h, grid_w = cx.shape\n",
        "    y_grid, x_grid = torch.meshgrid(\n",
        "        torch.arange(grid_h, device=det_output.device),\n",
        "        torch.arange(grid_w, device=det_output.device),\n",
        "        indexing='ij'\n",
        "    )\n",
        "\n",
        "    # 转换为绝对坐标（假设输入图像尺寸为 512x512）\n",
        "    scale = 512 / grid_h  # 特征图到原图的缩放比例\n",
        "    x1 = (x_grid + cx - w/2) * scale\n",
        "    y1 = (y_grid + cy - h/2) * scale\n",
        "    x2 = (x_grid + cx + w/2) * scale\n",
        "    y2 = (y_grid + cy + h/2) * scale\n",
        "\n",
        "    # 展平所有预测\n",
        "    boxes = torch.stack([x1, y1, x2, y2], dim=-1).reshape(-1, 4)  # [H*W, 4]\n",
        "    confidences = conf.reshape(-1)                                # [H*W]\n",
        "    class_ids = cls_probs.permute(1,2,0).reshape(-1, num_classes).argmax(dim=1)  # [H*W]\n",
        "\n",
        "    # 过滤低置信度预测\n",
        "    mask = confidences > conf_threshold\n",
        "    return boxes[mask], confidences[mask], class_ids[mask]\n",
        "\n",
        "def compute_map(predictions, targets, iou_threshold=0.5):\n",
        "    \"\"\"\n",
        "    計算mAP指標\n",
        "    \"\"\"\n",
        "    # 收集所有類別的AP\n",
        "    all_aps = []\n",
        "\n",
        "    # 獲取所有類別\n",
        "    all_classes = set()\n",
        "    for target in targets:\n",
        "        if 'labels' in target:\n",
        "            all_classes.update(target['labels'].tolist())\n",
        "\n",
        "    for class_id in all_classes:\n",
        "        # 收集該類別的所有預測和真值\n",
        "        class_predictions = []\n",
        "        class_targets = []\n",
        "\n",
        "        for i, (pred, target) in enumerate(zip(predictions, targets)):\n",
        "            # 預測中的該類別\n",
        "            if len(pred['labels']) > 0:\n",
        "                class_mask = pred['labels'] == class_id\n",
        "                if class_mask.any():\n",
        "                    class_predictions.append({\n",
        "                        'image_id': i,\n",
        "                        'boxes': pred['boxes'][class_mask],\n",
        "                        'scores': pred['scores'][class_mask]\n",
        "                    })\n",
        "\n",
        "            # 真值中的該類別\n",
        "            if 'labels' in target and len(target['labels']) > 0:\n",
        "                gt_mask = target['labels'] == class_id\n",
        "                if gt_mask.any():\n",
        "                    class_targets.append({\n",
        "                        'image_id': i,\n",
        "                        'boxes': target['boxes'][gt_mask]\n",
        "                    })\n",
        "\n",
        "        # 計算該類別的AP\n",
        "        ap = compute_ap(class_predictions, class_targets, iou_threshold)\n",
        "        all_aps.append(ap)\n",
        "\n",
        "    return np.mean(all_aps) if all_aps else 0.0\n",
        "\n",
        "def compute_ap(predictions, targets, iou_threshold):\n",
        "    \"\"\"\n",
        "    計算單一類別的Average Precision\n",
        "    \"\"\"\n",
        "    if not predictions or not targets:\n",
        "        return 0.0\n",
        "\n",
        "    # 按分數排序預測\n",
        "    all_pred_boxes = []\n",
        "    all_pred_scores = []\n",
        "    all_pred_image_ids = []\n",
        "\n",
        "    for pred in predictions:\n",
        "        for box, score in zip(pred['boxes'], pred['scores']):\n",
        "            all_pred_boxes.append(box)\n",
        "            all_pred_image_ids.append(pred['image_id'])\n",
        "\n",
        "    all_pred_scores = [pred['scores'].cpu().numpy() for pred in predictions]\n",
        "    all_pred_scores = np.concatenate(all_pred_scores)\n",
        "\n",
        "\n",
        "    if all_pred_scores.size == 0:  # 使用.size替代直接布林判斷\n",
        "        return 0.0\n",
        "\n",
        "    # 排序\n",
        "    sorted_indices = np.argsort(all_pred_scores)[::-1]\n",
        "\n",
        "    # 建立ground truth字典\n",
        "    gt_dict = defaultdict(list)\n",
        "    for target in targets:\n",
        "        gt_dict[target['image_id']].extend(target['boxes'])\n",
        "\n",
        "    # 計算precision和recall\n",
        "    tp = np.zeros(len(sorted_indices))\n",
        "    fp = np.zeros(len(sorted_indices))\n",
        "\n",
        "    for i, idx in enumerate(sorted_indices):\n",
        "        pred_box = all_pred_boxes[idx]\n",
        "        image_id = all_pred_image_ids[idx]\n",
        "\n",
        "        if image_id in gt_dict:\n",
        "            gt_boxes = gt_dict[image_id]\n",
        "            ious = [compute_iou_boxes(pred_box, gt_box) for gt_box in gt_boxes]\n",
        "            max_iou = max(ious) if ious else 0\n",
        "\n",
        "            if max_iou >= iou_threshold:\n",
        "                tp[i] = 1\n",
        "            else:\n",
        "                fp[i] = 1\n",
        "        else:\n",
        "            fp[i] = 1\n",
        "\n",
        "    # 累積和\n",
        "    tp_cumsum = np.cumsum(tp)\n",
        "    fp_cumsum = np.cumsum(fp)\n",
        "\n",
        "    # 計算precision和recall\n",
        "    total_gt = sum(len(target['boxes']) for target in targets)\n",
        "    recalls = tp_cumsum / max(total_gt, 1)\n",
        "    precisions = tp_cumsum / (tp_cumsum + fp_cumsum + 1e-8)\n",
        "\n",
        "    # 計算AP (使用11點插值)\n",
        "    ap = 0\n",
        "    for t in np.arange(0, 1.1, 0.1):\n",
        "        if np.sum(recalls >= t) == 0:\n",
        "            p = 0\n",
        "        else:\n",
        "            p = np.max(precisions[recalls >= t])\n",
        "        ap += p / 11\n",
        "\n",
        "    return ap\n",
        "\n",
        "def compute_iou_boxes(box1, box2):\n",
        "    \"\"\"\n",
        "    計算兩個bounding box的IoU\n",
        "    \"\"\"\n",
        "    x1_1, y1_1, x2_1, y2_1 = box1\n",
        "    x1_2, y1_2, x2_2, y2_2 = box2\n",
        "\n",
        "    # 計算交集\n",
        "    x1_inter = max(x1_1, x1_2)\n",
        "    y1_inter = max(y1_1, y1_2)\n",
        "    x2_inter = min(x2_1, x2_2)\n",
        "    y2_inter = min(y2_1, y2_2)\n",
        "\n",
        "    if x2_inter <= x1_inter or y2_inter <= y1_inter:\n",
        "        return 0.0\n",
        "\n",
        "    inter_area = (x2_inter - x1_inter) * (y2_inter - y1_inter)\n",
        "\n",
        "    # 計算聯集\n",
        "    area1 = (x2_1 - x1_1) * (y2_1 - y1_1)\n",
        "    area2 = (x2_2 - x1_2) * (y2_2 - y1_2)\n",
        "    union_area = area1 + area2 - inter_area\n",
        "\n",
        "    return inter_area / union_area if union_area > 0 else 0.0\n"
      ],
      "metadata": {
        "id": "lkPqiVGw0A20"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_classification(model, dataloader, device):\n",
        "    \"\"\"\n",
        "    評估分類任務的Top-1準確率\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    top5_correct = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, target in dataloader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "\n",
        "            # 模型輸出\n",
        "            _, _, cls_out = model(data)\n",
        "\n",
        "            # Top-1準確率\n",
        "            _, predicted = torch.max(cls_out, 1)\n",
        "            total += target.size(0)\n",
        "            correct += (predicted == target).sum().item()\n",
        "\n",
        "            # Top-5準確率 (如果類別數>=5)\n",
        "            if cls_out.size(1) >= 5:\n",
        "                _, top5_pred = torch.topk(cls_out, 5, dim=1)\n",
        "                top5_correct += sum([target[i] in top5_pred[i] for i in range(target.size(0))])\n",
        "\n",
        "    top1_accuracy = 100. * correct / total\n",
        "    top5_accuracy = 100. * top5_correct / total if cls_out.size(1) >= 5 else top1_accuracy\n",
        "\n",
        "    return top1_accuracy, top5_accuracy\n",
        "\n",
        "def evaluate_classification_detailed(model, dataloader, device, num_classes=10):\n",
        "    \"\"\"\n",
        "    詳細的分類評估，包含每類別的準確率\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    class_correct = list(0. for i in range(num_classes))\n",
        "    class_total = list(0. for i in range(num_classes))\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, target in dataloader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "\n",
        "            _, _, cls_out = model(data)\n",
        "            _, predicted = torch.max(cls_out, 1)\n",
        "\n",
        "            c = (predicted == target).squeeze()\n",
        "            for i in range(target.size(0)):\n",
        "                label = target[i]\n",
        "                class_correct[label] += c[i].item()\n",
        "                class_total[label] += 1\n",
        "\n",
        "    # 計算每類別準確率\n",
        "    class_accuracies = []\n",
        "    for i in range(num_classes):\n",
        "        if class_total[i] > 0:\n",
        "            accuracy = 100 * class_correct[i] / class_total[i]\n",
        "            class_accuracies.append(accuracy)\n",
        "            print(f'Class {i}: {accuracy:.2f}%')\n",
        "        else:\n",
        "            class_accuracies.append(0.0)\n",
        "\n",
        "    overall_accuracy = 100 * sum(class_correct) / sum(class_total)\n",
        "    return overall_accuracy, class_accuracies\n"
      ],
      "metadata": {
        "id": "WI2yUmsc0Der"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def detection_collate(batch):\n",
        "    images = []\n",
        "    targets = []\n",
        "\n",
        "    for image, target in batch:\n",
        "        images.append(image)\n",
        "        targets.append(target)\n",
        "\n",
        "    images = torch.stack(images, dim=0)\n",
        "    return images, targets"
      ],
      "metadata": {
        "id": "SqMsRNtC7B2_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"  # 顯示具體錯誤位置\n",
        "os.environ['TORCH_USE_CUDA_DSA'] = \"1\"    # 啟用設備端斷言\n",
        "\n",
        "def main():\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = MultiTaskModel().to(device)\n",
        "    trainer = MultiTaskTrainer(model, device)\n",
        "\n",
        "    data_root = \"data\"  # 你的數據根目錄\n",
        "\n",
        "    # 創建不同任務的數據集\n",
        "    train_seg_dataset = MultiTaskDataset(\n",
        "        data_root=data_root,\n",
        "        task_type='segmentation',\n",
        "        split='train',\n",
        "        transform=get_transforms('segmentation', 'train')\n",
        "    )\n",
        "\n",
        "    train_det_dataset = MultiTaskDataset(\n",
        "        data_root=data_root,\n",
        "        task_type='detection',\n",
        "        split='train',\n",
        "        transform=get_transforms('detection', 'train')\n",
        "    )\n",
        "\n",
        "    train_cls_dataset = MultiTaskDataset(\n",
        "        data_root=data_root,\n",
        "        task_type='classification',\n",
        "        split='train',\n",
        "        transform=get_transforms('classification', 'train')\n",
        "    )\n",
        "\n",
        "    # 創建 DataLoader\n",
        "    train_seg_loader = DataLoader(train_seg_dataset, batch_size=8, shuffle=True)\n",
        "    train_det_loader = DataLoader(train_det_dataset, batch_size=8, shuffle=True,  collate_fn=detection_collate)\n",
        "    train_cls_loader = DataLoader(train_cls_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "    print(f\"分割數據集: {len(train_seg_dataset)} 樣本\")\n",
        "    print(f\"檢測數據集: {len(train_det_dataset)} 樣本\")\n",
        "    print(f\"分類數據集: {len(train_cls_dataset)} 樣本\")\n",
        "\n",
        "    # 測試載入一個批次\n",
        "    seg_batch = next(iter(train_seg_loader))\n",
        "    det_batch = next(iter(train_det_loader))\n",
        "    cls_batch = next(iter(train_cls_loader))\n",
        "\n",
        "    print(f\"分割批次 - 圖片: {seg_batch[0].shape}, 標籤: {seg_batch[1].shape}\")\n",
        "    print(f\"檢測批次 - 圖片: {det_batch[0].shape}\")\n",
        "    print(f\"分類批次 - 圖片: {cls_batch[0].shape}, 標籤: {cls_batch[1].shape}\")\n",
        "\n",
        "\n",
        "    # 檢查模型參數數量\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    print(f'Total parameters: {total_params/1e6:.2f}M')\n",
        "\n",
        "    # Stage 1: 訓練分割任務\n",
        "    print(\"Stage 1: Training Segmentation\")\n",
        "    trainer.train_stage(train_seg_loader, 'segmentation')\n",
        "\n",
        "    # 記錄基準性能\n",
        "    mIoU_base = evaluate_segmentation(model, train_seg_loader, device)\n",
        "    print(f\"Segmentation baseline mIoU: {mIoU_base:.4f}\")\n",
        "\n",
        "    # 設定EWC\n",
        "    trainer.set_ewc({'seg': train_seg_loader})\n",
        "\n",
        "    # Stage 2: 訓練檢測任務\n",
        "    print(\"Stage 2: Training Detection\")\n",
        "    trainer.train_stage(train_det_loader, 'detection', num_epochs=100)\n",
        "\n",
        "    mAP_base = evaluate_detection(model, train_det_loader, device)\n",
        "    print(f\"Detection baseline mAP: {mAP_base:.4f}\")\n",
        "\n",
        "    trainer.set_ewc({'seg': train_seg_loader, 'det': train_det_loader})\n",
        "\n",
        "    # Stage 3: 訓練分類任務\n",
        "    print(\"Stage 3: Training Classification\")\n",
        "    trainer.train_stage(train_cls_loader, 'classification')\n",
        "\n",
        "    # 最終評估\n",
        "    final_mIoU = evaluate_segmentation(model, train_seg_loader, device)\n",
        "    final_mAP = evaluate_detection(model, train_det_loader, device)\n",
        "    final_acc = evaluate_classification(model, train_cls_loader, device)\n",
        "\n",
        "    print(f\"Final Results:\")\n",
        "    print(f\"mIoU: {final_mIoU:.4f} (drop: {mIoU_base - final_mIoU:.4f})\")\n",
        "    print(f\"mAP: {final_mAP:.4f}\")\n",
        "    print(f\"Top-1 Accuracy: {final_acc[0]:.4f}\")\n",
        "\n",
        "    # 儲存模型\n",
        "    torch.save(model.state_dict(), 'multitask_model.pt')\n",
        "    print(\"Model saved.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EN8SH4ZdSZS1",
        "outputId": "b54b93a7-06be-4d94-bb60-0f9c3946f87c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V3_Small_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V3_Small_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "載入分割樣本: 240 個\n",
            "loading annotations into memory...\n",
            "Done (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "載入檢測樣本: 240 個\n",
            "載入分類樣本: 240 個\n",
            "分割數據集: 240 樣本\n",
            "檢測數據集: 240 樣本\n",
            "分類數據集: 240 樣本\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-4199667699>:176: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  boxes = torch.tensor(boxes, dtype=torch.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "分割批次 - 圖片: torch.Size([8, 3, 512, 512]), 標籤: torch.Size([8, 512, 512])\n",
            "檢測批次 - 圖片: torch.Size([8, 3, 512, 512])\n",
            "分類批次 - 圖片: torch.Size([32, 3, 512, 512]), 標籤: torch.Size([32])\n",
            "Total parameters: 3.15M\n",
            "Stage 1: Training Segmentation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training segmentation:   2%|▏         | 1/60 [00:30<29:54, 30.42s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "segmentation Epoch 1/60, Loss: 2.3282\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining segmentation:   3%|▎         | 2/60 [01:00<29:19, 30.34s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "segmentation Epoch 2/60, Loss: 1.7287\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining segmentation:   5%|▌         | 3/60 [01:30<28:45, 30.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "segmentation Epoch 3/60, Loss: 1.5312\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining segmentation:   7%|▋         | 4/60 [02:01<28:16, 30.29s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "segmentation Epoch 4/60, Loss: 1.3623\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining segmentation:   8%|▊         | 5/60 [02:31<27:43, 30.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "segmentation Epoch 5/60, Loss: 1.2260\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining segmentation:  10%|█         | 6/60 [03:01<27:15, 30.29s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "segmentation Epoch 6/60, Loss: 1.1106\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining segmentation:  12%|█▏        | 7/60 [03:32<26:44, 30.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "segmentation Epoch 7/60, Loss: 0.9542\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining segmentation:  13%|█▎        | 8/60 [04:02<26:14, 30.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "segmentation Epoch 8/60, Loss: 0.8482\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining segmentation:  15%|█▌        | 9/60 [04:32<25:43, 30.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "segmentation Epoch 9/60, Loss: 0.7671\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining segmentation:  17%|█▋        | 10/60 [05:02<25:11, 30.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "segmentation Epoch 10/60, Loss: 0.7078\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining segmentation:  18%|█▊        | 11/60 [05:33<24:42, 30.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "segmentation Epoch 11/60, Loss: 0.6311\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining segmentation:  20%|██        | 12/60 [06:03<24:11, 30.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "segmentation Epoch 12/60, Loss: 0.5716\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining segmentation:  22%|██▏       | 13/60 [06:33<23:42, 30.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "segmentation Epoch 13/60, Loss: 0.5344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining segmentation:  23%|██▎       | 14/60 [07:03<23:10, 30.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "segmentation Epoch 14/60, Loss: 0.5084\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining segmentation:  25%|██▌       | 15/60 [07:33<22:40, 30.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "segmentation Epoch 15/60, Loss: 0.4500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining segmentation:  27%|██▋       | 16/60 [08:04<22:10, 30.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "segmentation Epoch 16/60, Loss: 0.4104\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining segmentation:  28%|██▊       | 17/60 [08:34<21:41, 30.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "segmentation Epoch 17/60, Loss: 0.4140\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining segmentation:  30%|███       | 18/60 [09:04<21:09, 30.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "segmentation Epoch 18/60, Loss: 0.3587\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining segmentation:  32%|███▏      | 19/60 [09:34<20:39, 30.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "segmentation Epoch 19/60, Loss: 0.3454\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining segmentation:  33%|███▎      | 20/60 [10:05<20:08, 30.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "segmentation Epoch 20/60, Loss: 0.3111\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining segmentation:  35%|███▌      | 21/60 [10:35<19:40, 30.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "segmentation Epoch 21/60, Loss: 0.2970\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining segmentation:  37%|███▋      | 22/60 [11:05<19:10, 30.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "segmentation Epoch 22/60, Loss: 0.2922\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining segmentation:  38%|███▊      | 23/60 [11:35<18:39, 30.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "segmentation Epoch 23/60, Loss: 0.2654\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining segmentation:  40%|████      | 24/60 [12:06<18:07, 30.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "segmentation Epoch 24/60, Loss: 0.2570\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining segmentation:  42%|████▏     | 25/60 [12:36<17:37, 30.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "segmentation Epoch 25/60, Loss: 0.2386\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining segmentation:  43%|████▎     | 26/60 [13:06<17:07, 30.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "segmentation Epoch 26/60, Loss: 0.2262\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining segmentation:  45%|████▌     | 27/60 [13:36<16:36, 30.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "segmentation Epoch 27/60, Loss: 0.2258\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining segmentation:  47%|████▋     | 28/60 [14:06<16:06, 30.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "segmentation Epoch 28/60, Loss: 0.2107\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining segmentation:  48%|████▊     | 29/60 [14:37<15:35, 30.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "segmentation Epoch 29/60, Loss: 0.1943\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining segmentation:  50%|█████     | 30/60 [15:07<15:05, 30.18s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "segmentation Epoch 30/60, Loss: 0.1934\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining segmentation:  52%|█████▏    | 31/60 [15:37<14:36, 30.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "segmentation Epoch 31/60, Loss: 0.1910\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining segmentation:  53%|█████▎    | 32/60 [16:07<14:06, 30.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "segmentation Epoch 32/60, Loss: 0.1820\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining segmentation:  55%|█████▌    | 33/60 [16:38<13:36, 30.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "segmentation Epoch 33/60, Loss: 0.1843\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining segmentation:  57%|█████▋    | 34/60 [17:08<13:05, 30.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "segmentation Epoch 34/60, Loss: 0.1656\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining segmentation:  58%|█████▊    | 35/60 [17:38<12:34, 30.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "segmentation Epoch 35/60, Loss: 0.1699\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining segmentation:  60%|██████    | 36/60 [18:08<12:05, 30.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "segmentation Epoch 36/60, Loss: 0.1589\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining segmentation:  62%|██████▏   | 37/60 [18:38<11:34, 30.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "segmentation Epoch 37/60, Loss: 0.1429\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining segmentation:  63%|██████▎   | 38/60 [19:09<11:04, 30.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "segmentation Epoch 38/60, Loss: 0.1484\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining segmentation:  65%|██████▌   | 39/60 [19:39<10:34, 30.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "segmentation Epoch 39/60, Loss: 0.1390\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining segmentation:  67%|██████▋   | 40/60 [20:09<10:04, 30.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "segmentation Epoch 40/60, Loss: 0.1470\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining segmentation:  68%|██████▊   | 41/60 [20:39<09:33, 30.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "segmentation Epoch 41/60, Loss: 0.1434\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining segmentation:  70%|███████   | 42/60 [21:09<09:04, 30.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "segmentation Epoch 42/60, Loss: 0.1440\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining segmentation:  72%|███████▏  | 43/60 [21:40<08:33, 30.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "segmentation Epoch 43/60, Loss: 0.1295\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining segmentation:  73%|███████▎  | 44/60 [22:10<08:04, 30.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "segmentation Epoch 44/60, Loss: 0.1192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining segmentation:  75%|███████▌  | 45/60 [22:40<07:33, 30.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "segmentation Epoch 45/60, Loss: 0.1292\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining segmentation:  77%|███████▋  | 46/60 [23:11<07:03, 30.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "segmentation Epoch 46/60, Loss: 0.1160\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining segmentation:  78%|███████▊  | 47/60 [23:41<06:33, 30.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "segmentation Epoch 47/60, Loss: 0.1132\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining segmentation:  80%|████████  | 48/60 [24:11<06:02, 30.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "segmentation Epoch 48/60, Loss: 0.1154\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining segmentation:  82%|████████▏ | 49/60 [24:41<05:32, 30.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "segmentation Epoch 49/60, Loss: 0.1145\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining segmentation:  83%|████████▎ | 50/60 [25:12<05:02, 30.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "segmentation Epoch 50/60, Loss: 0.1131\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining segmentation:  85%|████████▌ | 51/60 [25:42<04:32, 30.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "segmentation Epoch 51/60, Loss: 0.1040\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining segmentation:  87%|████████▋ | 52/60 [26:12<04:01, 30.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "segmentation Epoch 52/60, Loss: 0.1004\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining segmentation:  88%|████████▊ | 53/60 [26:42<03:31, 30.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "segmentation Epoch 53/60, Loss: 0.0953\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining segmentation:  90%|█████████ | 54/60 [27:12<03:01, 30.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "segmentation Epoch 54/60, Loss: 0.0949\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining segmentation:  92%|█████████▏| 55/60 [27:43<02:31, 30.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "segmentation Epoch 55/60, Loss: 0.0923\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining segmentation:  93%|█████████▎| 56/60 [28:13<02:00, 30.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "segmentation Epoch 56/60, Loss: 0.0913\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining segmentation:  95%|█████████▌| 57/60 [28:43<01:30, 30.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "segmentation Epoch 57/60, Loss: 0.0947\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining segmentation:  97%|█████████▋| 58/60 [29:13<01:00, 30.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "segmentation Epoch 58/60, Loss: 0.0914\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining segmentation:  98%|█████████▊| 59/60 [29:44<00:30, 30.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "segmentation Epoch 59/60, Loss: 0.0918\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training segmentation: 100%|██████████| 60/60 [30:14<00:00, 30.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "segmentation Epoch 60/60, Loss: 0.0880\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Segmentation baseline mIoU: 0.2154\n",
            "正在計算 seg 任務的Fisher信息...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "seg: 100%|██████████| 30/30 [00:30<00:00,  1.01s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stage 2: Training Detection\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training detection:   1%|          | 1/100 [00:08<13:27,  8.16s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "detection Epoch 1/100, Loss: 88789.5727\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining detection:   2%|▏         | 2/100 [00:16<13:30,  8.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "detection Epoch 2/100, Loss: 83979.6906\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining detection:   3%|▎         | 3/100 [00:24<13:14,  8.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "detection Epoch 3/100, Loss: 81314.8398\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining detection:   4%|▍         | 4/100 [00:33<13:14,  8.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "detection Epoch 4/100, Loss: 79668.5777\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining detection:   5%|▌         | 5/100 [00:41<13:17,  8.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "detection Epoch 5/100, Loss: 78203.3870\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining detection:   6%|▌         | 6/100 [00:49<12:53,  8.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "detection Epoch 6/100, Loss: 76852.6362\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining detection:   7%|▋         | 7/100 [00:58<12:53,  8.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "detection Epoch 7/100, Loss: 75410.5913\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining detection:   8%|▊         | 8/100 [01:06<12:51,  8.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "detection Epoch 8/100, Loss: 74295.8402\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining detection:   9%|▉         | 9/100 [01:14<12:27,  8.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "detection Epoch 9/100, Loss: 73085.6979\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining detection:  10%|█         | 10/100 [01:22<12:27,  8.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "detection Epoch 10/100, Loss: 71582.7035\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining detection:  11%|█         | 11/100 [01:31<12:25,  8.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "detection Epoch 11/100, Loss: 70804.7913\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining detection:  12%|█▏        | 12/100 [01:39<12:03,  8.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "detection Epoch 12/100, Loss: 69773.1457\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining detection:  13%|█▎        | 13/100 [01:47<11:59,  8.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "detection Epoch 13/100, Loss: 68810.7745\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining detection:  14%|█▍        | 14/100 [01:56<11:54,  8.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "detection Epoch 14/100, Loss: 67757.2260\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining detection:  15%|█▌        | 15/100 [02:03<11:33,  8.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "detection Epoch 15/100, Loss: 66756.3233\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining detection:  16%|█▌        | 16/100 [02:12<11:29,  8.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "detection Epoch 16/100, Loss: 65863.0388\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining detection:  17%|█▋        | 17/100 [02:20<11:24,  8.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "detection Epoch 17/100, Loss: 65103.1055\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining detection:  18%|█▊        | 18/100 [02:28<11:07,  8.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "detection Epoch 18/100, Loss: 64215.3896\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining detection:  19%|█▉        | 19/100 [02:37<11:12,  8.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "detection Epoch 19/100, Loss: 63115.0441\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining detection:  20%|██        | 20/100 [02:45<11:10,  8.38s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "detection Epoch 20/100, Loss: 62582.9613\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining detection:  21%|██        | 21/100 [02:53<10:50,  8.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "detection Epoch 21/100, Loss: 61682.8177\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining detection:  22%|██▏       | 22/100 [03:02<10:47,  8.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "detection Epoch 22/100, Loss: 60902.5546\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining detection:  23%|██▎       | 23/100 [03:10<10:42,  8.34s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "detection Epoch 23/100, Loss: 60111.0460\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining detection:  24%|██▍       | 24/100 [03:18<10:24,  8.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "detection Epoch 24/100, Loss: 59134.4471\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining detection:  25%|██▌       | 25/100 [03:26<10:20,  8.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "detection Epoch 25/100, Loss: 58545.3872\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining detection:  26%|██▌       | 26/100 [03:35<10:16,  8.34s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "detection Epoch 26/100, Loss: 57726.4642\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining detection:  27%|██▋       | 27/100 [03:43<10:00,  8.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "detection Epoch 27/100, Loss: 57080.8973\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining detection:  28%|██▊       | 28/100 [03:51<09:53,  8.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "detection Epoch 28/100, Loss: 56192.7548\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining detection:  29%|██▉       | 29/100 [03:59<09:49,  8.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "detection Epoch 29/100, Loss: 55518.8898\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining detection:  30%|███       | 30/100 [04:08<09:39,  8.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "detection Epoch 30/100, Loss: 54782.9815\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining detection:  31%|███       | 31/100 [04:16<09:29,  8.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "detection Epoch 31/100, Loss: 54517.3759\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining detection:  32%|███▏      | 32/100 [04:24<09:24,  8.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "detection Epoch 32/100, Loss: 53500.5818\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining detection:  33%|███▎      | 33/100 [04:32<09:12,  8.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "detection Epoch 33/100, Loss: 52699.5732\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining detection:  34%|███▍      | 34/100 [04:40<08:59,  8.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "detection Epoch 34/100, Loss: 51943.0987\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining detection:  35%|███▌      | 35/100 [04:49<08:55,  8.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "detection Epoch 35/100, Loss: 51184.3846\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining detection:  36%|███▌      | 36/100 [04:57<08:49,  8.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "detection Epoch 36/100, Loss: 50609.2373\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining detection:  37%|███▋      | 37/100 [05:05<08:36,  8.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "detection Epoch 37/100, Loss: 49733.4247\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining detection:  38%|███▊      | 38/100 [05:14<08:32,  8.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "detection Epoch 38/100, Loss: 49090.2128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining detection:  39%|███▉      | 39/100 [05:22<08:26,  8.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "detection Epoch 39/100, Loss: 48636.8113\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining detection:  40%|████      | 40/100 [05:30<08:08,  8.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "detection Epoch 40/100, Loss: 47882.7477\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining detection:  41%|████      | 41/100 [05:38<08:06,  8.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "detection Epoch 41/100, Loss: 47442.5134\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining detection:  42%|████▏     | 42/100 [05:47<08:00,  8.29s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "detection Epoch 42/100, Loss: 46641.9528\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining detection:  43%|████▎     | 43/100 [05:54<07:43,  8.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "detection Epoch 43/100, Loss: 46209.1201\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining detection:  44%|████▍     | 44/100 [06:03<07:41,  8.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "detection Epoch 44/100, Loss: 45430.6288\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining detection:  45%|████▌     | 45/100 [06:11<07:35,  8.29s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "detection Epoch 45/100, Loss: 44943.2676\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining detection:  46%|████▌     | 46/100 [06:19<07:18,  8.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "detection Epoch 46/100, Loss: 44623.1608\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining detection:  47%|████▋     | 47/100 [06:27<07:13,  8.18s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "detection Epoch 47/100, Loss: 44015.2195\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining detection:  48%|████▊     | 48/100 [06:36<07:09,  8.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "detection Epoch 48/100, Loss: 43493.7066\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining detection:  49%|████▉     | 49/100 [06:44<06:53,  8.11s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "detection Epoch 49/100, Loss: 42690.3494\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining detection:  50%|█████     | 50/100 [06:52<06:51,  8.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "detection Epoch 50/100, Loss: 42308.7622\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining detection:  51%|█████     | 51/100 [07:01<06:47,  8.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "detection Epoch 51/100, Loss: 41653.0047\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining detection:  52%|█████▏    | 52/100 [07:08<06:32,  8.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "detection Epoch 52/100, Loss: 41412.4721\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining detection:  53%|█████▎    | 53/100 [07:17<06:27,  8.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "detection Epoch 53/100, Loss: 40692.9938\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining detection:  54%|█████▍    | 54/100 [07:25<06:21,  8.29s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "detection Epoch 54/100, Loss: 40261.1289\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining detection:  55%|█████▌    | 55/100 [07:33<06:07,  8.16s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "detection Epoch 55/100, Loss: 39784.8995\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining detection:  56%|█████▌    | 56/100 [07:41<06:01,  8.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "detection Epoch 56/100, Loss: 39212.1604\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining detection:  57%|█████▋    | 57/100 [07:50<05:55,  8.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "detection Epoch 57/100, Loss: 38808.6773\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining detection:  58%|█████▊    | 58/100 [07:58<05:41,  8.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "detection Epoch 58/100, Loss: 38410.0374\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining detection:  59%|█████▉    | 59/100 [08:06<05:38,  8.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "detection Epoch 59/100, Loss: 38035.9291\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining detection:  60%|██████    | 60/100 [08:15<05:33,  8.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "detection Epoch 60/100, Loss: 37558.6655\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining detection:  61%|██████    | 61/100 [08:23<05:19,  8.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "detection Epoch 61/100, Loss: 37046.7529\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining detection:  62%|██████▏   | 62/100 [08:31<05:16,  8.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "detection Epoch 62/100, Loss: 36755.6509\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining detection:  63%|██████▎   | 63/100 [08:40<05:09,  8.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "detection Epoch 63/100, Loss: 36422.5391\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining detection:  64%|██████▍   | 64/100 [08:48<04:59,  8.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "detection Epoch 64/100, Loss: 36129.5602\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining detection:  65%|██████▌   | 65/100 [08:56<04:51,  8.34s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "detection Epoch 65/100, Loss: 35630.7193\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining detection:  66%|██████▌   | 66/100 [09:05<04:46,  8.43s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "detection Epoch 66/100, Loss: 35391.9792\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining detection:  67%|██████▋   | 67/100 [09:13<04:37,  8.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "detection Epoch 67/100, Loss: 34919.1454\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining detection:  68%|██████▊   | 68/100 [09:21<04:25,  8.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "detection Epoch 68/100, Loss: 34670.6452\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining detection:  69%|██████▉   | 69/100 [09:30<04:19,  8.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "detection Epoch 69/100, Loss: 34166.3217\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining detection:  70%|███████   | 70/100 [09:38<04:13,  8.44s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "detection Epoch 70/100, Loss: 33786.6117\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining detection:  71%|███████   | 71/100 [09:46<03:59,  8.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "detection Epoch 71/100, Loss: 33665.1252\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining detection:  72%|███████▏  | 72/100 [09:55<03:53,  8.34s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "detection Epoch 72/100, Loss: 33398.7689\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining detection:  73%|███████▎  | 73/100 [10:03<03:45,  8.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "detection Epoch 73/100, Loss: 33007.8867\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining detection:  74%|███████▍  | 74/100 [10:11<03:33,  8.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "detection Epoch 74/100, Loss: 32616.1794\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining detection:  75%|███████▌  | 75/100 [10:19<03:26,  8.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "detection Epoch 75/100, Loss: 32202.0765\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining detection:  76%|███████▌  | 76/100 [10:28<03:20,  8.36s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "detection Epoch 76/100, Loss: 31972.6919\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining detection:  77%|███████▋  | 77/100 [10:36<03:08,  8.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "detection Epoch 77/100, Loss: 31799.8475\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining detection:  78%|███████▊  | 78/100 [10:44<03:02,  8.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "detection Epoch 78/100, Loss: 31522.1243\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining detection:  79%|███████▉  | 79/100 [10:53<02:55,  8.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "detection Epoch 79/100, Loss: 31275.1944\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining detection:  80%|████████  | 80/100 [11:01<02:44,  8.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "detection Epoch 80/100, Loss: 31029.2341\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining detection:  81%|████████  | 81/100 [11:09<02:37,  8.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "detection Epoch 81/100, Loss: 30635.0790\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining detection:  82%|████████▏ | 82/100 [11:18<02:29,  8.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "detection Epoch 82/100, Loss: 30428.9001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining detection:  83%|████████▎ | 83/100 [11:25<02:18,  8.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "detection Epoch 83/100, Loss: 30227.1929\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining detection:  84%|████████▍ | 84/100 [11:34<02:12,  8.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "detection Epoch 84/100, Loss: 29942.8290\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining detection:  85%|████████▌ | 85/100 [11:42<02:04,  8.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "detection Epoch 85/100, Loss: 29549.9154\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining detection:  86%|████████▌ | 86/100 [11:50<01:54,  8.18s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "detection Epoch 86/100, Loss: 29527.6523\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining detection:  87%|████████▋ | 87/100 [11:59<01:47,  8.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "detection Epoch 87/100, Loss: 29186.8536\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining detection:  88%|████████▊ | 88/100 [12:07<01:40,  8.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "detection Epoch 88/100, Loss: 28956.4856\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining detection:  89%|████████▉ | 89/100 [12:15<01:31,  8.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "detection Epoch 89/100, Loss: 28821.4696\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining detection:  90%|█████████ | 90/100 [12:24<01:22,  8.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "detection Epoch 90/100, Loss: 28697.9924\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining detection:  91%|█████████ | 91/100 [12:32<01:14,  8.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "detection Epoch 91/100, Loss: 28522.6496\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining detection:  92%|█████████▏| 92/100 [12:40<01:06,  8.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "detection Epoch 92/100, Loss: 28249.5379\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining detection:  93%|█████████▎| 93/100 [12:49<00:57,  8.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "detection Epoch 93/100, Loss: 27955.7989\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining detection:  94%|█████████▍| 94/100 [12:57<00:50,  8.34s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "detection Epoch 94/100, Loss: 27642.7477\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining detection:  95%|█████████▌| 95/100 [13:05<00:41,  8.36s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "detection Epoch 95/100, Loss: 27611.2278\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining detection:  96%|█████████▌| 96/100 [13:13<00:32,  8.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "detection Epoch 96/100, Loss: 27489.8229\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining detection:  97%|█████████▋| 97/100 [13:22<00:24,  8.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "detection Epoch 97/100, Loss: 27261.2989\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining detection:  98%|█████████▊| 98/100 [13:30<00:16,  8.36s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "detection Epoch 98/100, Loss: 27112.3400\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining detection:  99%|█████████▉| 99/100 [13:38<00:08,  8.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "detection Epoch 99/100, Loss: 27034.0083\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training detection: 100%|██████████| 100/100 [13:47<00:00,  8.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "detection Epoch 100/100, Loss: 27019.0214\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detection baseline mAP: 0.0000\n",
            "正在計算 seg 任務的Fisher信息...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "seg: 100%|██████████| 30/30 [00:30<00:00,  1.02s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "正在計算 det 任務的Fisher信息...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "det: 100%|██████████| 30/30 [00:06<00:00,  4.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stage 3: Training Classification\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training classification:   2%|▏         | 1/60 [00:04<04:22,  4.45s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classification Epoch 1/60, Loss: 45078.6706\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining classification:   3%|▎         | 2/60 [00:08<04:01,  4.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classification Epoch 2/60, Loss: 19695.3963\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining classification:   5%|▌         | 3/60 [00:12<03:52,  4.07s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classification Epoch 3/60, Loss: 8468.5839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining classification:   7%|▋         | 4/60 [00:16<03:56,  4.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classification Epoch 4/60, Loss: 2686.2763\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining classification:   8%|▊         | 5/60 [00:20<03:46,  4.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classification Epoch 5/60, Loss: 1936.8615\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining classification:  10%|█         | 6/60 [00:24<03:39,  4.06s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classification Epoch 6/60, Loss: 556.5414\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining classification:  12%|█▏        | 7/60 [00:29<03:41,  4.18s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classification Epoch 7/60, Loss: 389.4873\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining classification:  13%|█▎        | 8/60 [00:33<03:33,  4.11s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classification Epoch 8/60, Loss: 119.0512\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining classification:  15%|█▌        | 9/60 [00:37<03:28,  4.08s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classification Epoch 9/60, Loss: 68.4999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining classification:  17%|█▋        | 10/60 [00:41<03:29,  4.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classification Epoch 10/60, Loss: 34.6069\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining classification:  18%|█▊        | 11/60 [00:45<03:21,  4.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classification Epoch 11/60, Loss: 13.7847\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining classification:  20%|██        | 12/60 [00:49<03:17,  4.11s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classification Epoch 12/60, Loss: 7.1919\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining classification:  22%|██▏       | 13/60 [00:53<03:16,  4.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classification Epoch 13/60, Loss: 4.6108\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining classification:  23%|██▎       | 14/60 [00:57<03:08,  4.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classification Epoch 14/60, Loss: 3.3444\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining classification:  25%|██▌       | 15/60 [01:01<03:04,  4.11s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classification Epoch 15/60, Loss: 2.6534\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining classification:  27%|██▋       | 16/60 [01:06<03:03,  4.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classification Epoch 16/60, Loss: 2.3895\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining classification:  28%|██▊       | 17/60 [01:10<02:57,  4.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classification Epoch 17/60, Loss: 2.2685\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining classification:  30%|███       | 18/60 [01:14<02:52,  4.11s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classification Epoch 18/60, Loss: 2.2125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining classification:  32%|███▏      | 19/60 [01:18<02:50,  4.16s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classification Epoch 19/60, Loss: 2.1880\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining classification:  33%|███▎      | 20/60 [01:22<02:43,  4.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classification Epoch 20/60, Loss: 2.1828\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining classification:  35%|███▌      | 21/60 [01:26<02:40,  4.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classification Epoch 21/60, Loss: 2.1703\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining classification:  37%|███▋      | 22/60 [01:31<02:37,  4.16s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classification Epoch 22/60, Loss: 2.1605\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining classification:  38%|███▊      | 23/60 [01:34<02:31,  4.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classification Epoch 23/60, Loss: 2.1515\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining classification:  40%|████      | 24/60 [01:39<02:28,  4.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classification Epoch 24/60, Loss: 2.1548\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining classification:  42%|████▏     | 25/60 [01:43<02:25,  4.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classification Epoch 25/60, Loss: 2.1251\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining classification:  43%|████▎     | 26/60 [01:47<02:19,  4.09s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classification Epoch 26/60, Loss: 2.0911\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining classification:  45%|████▌     | 27/60 [01:51<02:16,  4.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classification Epoch 27/60, Loss: 2.0767\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining classification:  47%|████▋     | 28/60 [01:55<02:13,  4.16s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classification Epoch 28/60, Loss: 2.0651\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining classification:  48%|████▊     | 29/60 [01:59<02:07,  4.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classification Epoch 29/60, Loss: 2.0691\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining classification:  50%|█████     | 30/60 [02:04<02:04,  4.16s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classification Epoch 30/60, Loss: 2.0629\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining classification:  52%|█████▏    | 31/60 [02:08<02:00,  4.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classification Epoch 31/60, Loss: 2.0489\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining classification:  53%|█████▎    | 32/60 [02:12<01:55,  4.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classification Epoch 32/60, Loss: 2.0332\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining classification:  55%|█████▌    | 33/60 [02:16<01:53,  4.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classification Epoch 33/60, Loss: 2.0291\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining classification:  57%|█████▋    | 34/60 [02:20<01:48,  4.16s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classification Epoch 34/60, Loss: 2.0251\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining classification:  58%|█████▊    | 35/60 [02:24<01:42,  4.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classification Epoch 35/60, Loss: 2.0266\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining classification:  60%|██████    | 36/60 [02:29<01:40,  4.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classification Epoch 36/60, Loss: 2.0072\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining classification:  62%|██████▏   | 37/60 [02:33<01:35,  4.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classification Epoch 37/60, Loss: 1.9984\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining classification:  63%|██████▎   | 38/60 [02:37<01:30,  4.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classification Epoch 38/60, Loss: 1.9854\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining classification:  65%|██████▌   | 39/60 [02:41<01:28,  4.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classification Epoch 39/60, Loss: 1.9821\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining classification:  67%|██████▋   | 40/60 [02:45<01:22,  4.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classification Epoch 40/60, Loss: 1.9925\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining classification:  68%|██████▊   | 41/60 [02:49<01:18,  4.11s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classification Epoch 41/60, Loss: 1.9791\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining classification:  70%|███████   | 42/60 [02:54<01:15,  4.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classification Epoch 42/60, Loss: 1.9736\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining classification:  72%|███████▏  | 43/60 [02:57<01:10,  4.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classification Epoch 43/60, Loss: 1.9621\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining classification:  73%|███████▎  | 44/60 [03:01<01:05,  4.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classification Epoch 44/60, Loss: 1.9530\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining classification:  75%|███████▌  | 45/60 [03:06<01:03,  4.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classification Epoch 45/60, Loss: 1.9381\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining classification:  77%|███████▋  | 46/60 [03:10<00:58,  4.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classification Epoch 46/60, Loss: 1.9282\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining classification:  78%|███████▊  | 47/60 [03:14<00:53,  4.11s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classification Epoch 47/60, Loss: 1.9140\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining classification:  80%|████████  | 48/60 [03:18<00:50,  4.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classification Epoch 48/60, Loss: 1.9186\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining classification:  82%|████████▏ | 49/60 [03:22<00:45,  4.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classification Epoch 49/60, Loss: 1.9093\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining classification:  83%|████████▎ | 50/60 [03:26<00:40,  4.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classification Epoch 50/60, Loss: 1.8968\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining classification:  85%|████████▌ | 51/60 [03:31<00:37,  4.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classification Epoch 51/60, Loss: 1.8785\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining classification:  87%|████████▋ | 52/60 [03:35<00:33,  4.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classification Epoch 52/60, Loss: 1.8671\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining classification:  88%|████████▊ | 53/60 [03:39<00:28,  4.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classification Epoch 53/60, Loss: 1.8758\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining classification:  90%|█████████ | 54/60 [03:43<00:25,  4.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classification Epoch 54/60, Loss: 1.8571\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining classification:  92%|█████████▏| 55/60 [03:47<00:20,  4.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classification Epoch 55/60, Loss: 1.8565\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining classification:  93%|█████████▎| 56/60 [03:51<00:16,  4.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classification Epoch 56/60, Loss: 1.8307\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining classification:  95%|█████████▌| 57/60 [03:56<00:12,  4.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classification Epoch 57/60, Loss: 1.8178\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining classification:  97%|█████████▋| 58/60 [04:00<00:08,  4.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classification Epoch 58/60, Loss: 1.8150\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining classification:  98%|█████████▊| 59/60 [04:04<00:04,  4.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classification Epoch 59/60, Loss: 1.8282\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training classification: 100%|██████████| 60/60 [04:08<00:00,  4.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classification Epoch 60/60, Loss: 1.7844\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Results:\n",
            "mIoU: 0.0000 (drop: 0.2154)\n",
            "mAP: 0.0000\n",
            "Top-1 Accuracy: 55.8333\n",
            "Model saved.\n"
          ]
        }
      ]
    }
  ]
}